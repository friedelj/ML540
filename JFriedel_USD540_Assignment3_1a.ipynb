{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd7P9+/o2TnoHrVjtBLLMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/ML540/blob/main/JFriedel_USD540_Assignment3_1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JFriedel                                                     Assignment 3_1                                                     6-2-25"
      ],
      "metadata": {
        "id": "p3C4foJ76yT9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CRaaEQl6vEM"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "original_boto3_version = boto3.__version__\n",
        "%pip install 'boto3>1.17.21'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.session import Session\n",
        "\n",
        "region = boto3.Session().region_name\n",
        "\n",
        "boto_session = boto3.Session(region_name=region)\n",
        "\n",
        "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
        "featurestore_runtime = boto_session.client(\n",
        "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
        ")\n",
        "\n",
        "feature_store_session = Session(\n",
        "    boto_session=boto_session,\n",
        "    sagemaker_client=sagemaker_client,\n",
        "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
        ")"
      ],
      "metadata": {
        "id": "-ablpC5x65mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the following to use a bucket of your choosing\n",
        "default_s3_bucket_name = feature_store_session.default_bucket()\n",
        "prefix = \"sagemaker-featurestore-asmt3\"\n",
        "\n",
        "print(default_s3_bucket_name)"
      ],
      "metadata": {
        "id": "Z1KNtEQs6-rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "\n",
        "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
        "role = get_execution_role()\n",
        "print(role)"
      ],
      "metadata": {
        "id": "gmJNW_am7ByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "fraud_detection_bucket_name = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\"housing_gmaps_data_raw.csv\")\n",
        "transaction_file_key = (\"housing.csv\")"
      ],
      "metadata": {
        "id": "bBIUNWWm7FZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------DATA CLEANING"
      ],
      "metadata": {
        "id": "7qKl_3jx7Mpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import re\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "xi1KW9NT7IVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df_housing = pd.read_csv('housing.csv')\n",
        "\n",
        "# Load the CSV file\n",
        "df_gmaps = pd.read_csv('housing_gmaps_data_raw.csv')"
      ],
      "metadata": {
        "id": "y34TwZWa7X2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge df_housing with df_gmaps on 'longitude' and 'latitude'\n",
        "df_housing = df_housing.merge(\n",
        "    df_gmaps[['longitude', 'latitude', 'postal_code']],\n",
        "    on=['longitude', 'latitude'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Save the updated df_housing to a CSV file\n",
        "df_housing.to_csv('housing_with_postal_code.csv', index=False)"
      ],
      "metadata": {
        "id": "oohiLnne7bZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "grbA_kdv7eJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the rounded average total_bedrooms for each postal_code\n",
        "avg_bedrooms_by_postal = df_housing.groupby('postal_code')['total_bedrooms'].mean().round()\n",
        "\n",
        "# Define a function to apply the group average to missing values\n",
        "def fill_bedrooms(row):\n",
        "    if pd.isna(row['total_bedrooms']):\n",
        "        return avg_bedrooms_by_postal.get(row['postal_code'], np.nan)\n",
        "    else:\n",
        "        return row['total_bedrooms']\n",
        "\n",
        "# Apply the function\n",
        "df_housing['total_bedrooms'] = df_housing.apply(fill_bedrooms, axis=1)\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_housing.to_csv('housing_bedrooms_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "4eHdJupi7hib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "9ezaFQmg7lSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate known and unknown postal_code rows\n",
        "known = df_housing[df_housing['postal_code'].notna()].copy()\n",
        "unknown = df_housing[df_housing['postal_code'].isna()].copy()\n",
        "\n",
        "# Build a KDTree from known locations\n",
        "tree = cKDTree(known[['latitude', 'longitude']])\n",
        "\n",
        "# Query the closest known point for each unknown point\n",
        "distances, indices = tree.query(unknown[['latitude', 'longitude']], k=1)\n",
        "\n",
        "# Assign the closest known postal_code to the unknown rows\n",
        "closest_postal_codes = known.iloc[indices]['postal_code'].values\n",
        "df_housing.loc[unknown.index, 'postal_code'] = closest_postal_codes\n",
        "\n",
        "# Save the updated DataFrame to CSV\n",
        "df_housing.to_csv('housing_postal_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "7Y_Wmks77oTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "ctZjbSma7sNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new column by dividing total_bedrooms by households and rounding the result\n",
        "df_housing['bedrooms_per_household'] = (df_housing['total_bedrooms'] / df_housing['households']).round()\n",
        "\n",
        "# Save the updated DataFrame to CSV\n",
        "df_housing.to_csv('housing_bedrooms_per_household.csv', index=False)"
      ],
      "metadata": {
        "id": "O5O7jbRc7vV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing.head(10)"
      ],
      "metadata": {
        "id": "Zcdu6iMP7zOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "Ebaw_Ziw72Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_gmaps.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_gmaps.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "zqCirg4C76et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if needed)\n",
        "df_gmaps['administrative_area_level_1-political'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values with 'California'\n",
        "df_gmaps['administrative_area_level_1-political'].fillna('California', inplace=True)\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "w_CfV3677-E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if needed)\n",
        "df_gmaps['postal_code_suffix'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values with 9999 and convert the column to integer\n",
        "df_gmaps['postal_code_suffix'] = df_gmaps['postal_code_suffix'].fillna(9999).astype(int)\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "WffcrHDR8DsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if needed)\n",
        "df_gmaps['street_number'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values with \"0\" (as a string to match text-based route values)\n",
        "df_gmaps['street_number'] = df_gmaps['street_number'].fillna('0')\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "_3FV5rmx8HPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate rows with and without the target value\n",
        "df_missing = df_gmaps[df_gmaps['administrative_area_level_2-political'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['administrative_area_level_2-political'].notna()]\n",
        "\n",
        "# Fit NearestNeighbors on known locations\n",
        "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
        "nn.fit(df_known[['latitude', 'longitude']])\n",
        "\n",
        "# Find closest matches for missing rows\n",
        "distances, indices = nn.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill missing values\n",
        "df_gmaps.loc[df_missing.index, 'administrative_area_level_2-political'] = \\\n",
        "    df_known.iloc[indices.flatten()]['administrative_area_level_2-political'].values\n",
        "\n",
        "# Save the modified DataFrame to a CSV\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "OR8ByxU28KxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the route from the address\n",
        "def extract_route(address):\n",
        "    if pd.isna(address):\n",
        "        return None\n",
        "    match = re.search(r'\\d+\\s+([^,]+)', address)\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "# Replace empty strings with NaN if necessary\n",
        "df_gmaps['route'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing 'route' values\n",
        "df_gmaps['route'] = df_gmaps.apply(\n",
        "    lambda row: extract_route(row['address']) if pd.isna(row['route']) else row['route'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "ZcDPe0It8OV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if applicable)\n",
        "df_gmaps['route'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values in 'route' with the default string\n",
        "df_gmaps['route'] = df_gmaps['route'].fillna('strret name not filled in')\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "sEfUx-zt8SE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert empty strings to NaN\n",
        "df_gmaps['locality-political'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Identify rows with missing and non-missing 'locality-political'\n",
        "df_missing = df_gmaps[df_gmaps['locality-political'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['locality-political'].notna()]\n",
        "\n",
        "# Use NearestNeighbors to find closest known point\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df_known[['latitude', 'longitude']])\n",
        "distances, indices = nbrs.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill missing 'locality-political' with nearest neighbor's value\n",
        "for i, idx in enumerate(df_missing.index):\n",
        "    nearest_index = df_known.index[indices[i][0]]\n",
        "    df_gmaps.at[idx, 'locality-political'] = df_known.at[nearest_index, 'locality-political']\n",
        "\n",
        "# Save the modified DataFrame\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "Dv97p3c78VQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert empty strings to NaN\n",
        "df_gmaps['postal_code'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Split data into rows with missing and non-missing postal_code\n",
        "df_missing = df_gmaps[df_gmaps['postal_code'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['postal_code'].notna()]\n",
        "\n",
        "# Fit NearestNeighbors using non-missing data\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df_known[['latitude', 'longitude']])\n",
        "distances, indices = nbrs.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill missing postal_code values from nearest neighbor\n",
        "for i, idx in enumerate(df_missing.index):\n",
        "    nearest_idx = df_known.index[indices[i][0]]\n",
        "    df_gmaps.at[idx, 'postal_code'] = df_known.at[nearest_idx, 'postal_code']\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "_Ngk_o9c8Y1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert empty strings to NaN\n",
        "df_gmaps['neighborhood-political'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Split the DataFrame into rows with and without missing 'neighborhood-political'\n",
        "df_missing = df_gmaps[df_gmaps['neighborhood-political'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['neighborhood-political'].notna()]\n",
        "\n",
        "# Fit NearestNeighbors model on known values\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df_known[['latitude', 'longitude']])\n",
        "distances, indices = nbrs.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill in missing 'neighborhood-political' values\n",
        "for i, idx in enumerate(df_missing.index):\n",
        "    nearest_idx = df_known.index[indices[i][0]]\n",
        "    df_gmaps.at[idx, 'neighborhood-political'] = df_known.at[nearest_idx, 'neighborhood-political']\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "GijXEh3_8c5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_gmaps.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_gmaps.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "aCWdn3Zj8gDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the first 12 columns\n",
        "df_gmaps = df_gmaps.iloc[:, :12]\n",
        "\n",
        "# Save the result to a new CSV\n",
        "df_gmaps.to_csv('housing_gmaps_data_trimmed.csv', index=False)\n",
        "\n",
        "# Display the result\n",
        "print(df_gmaps.head())"
      ],
      "metadata": {
        "id": "BXYC72aq8jiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_gmaps.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_gmaps.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "o0t1icKk8nHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gmaps.head(10)"
      ],
      "metadata": {
        "id": "0bos1w608qas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract unique values (including NaNs if needed) and create the new DataFrame\n",
        "df_neighborhood = pd.DataFrame({\n",
        "    'neighborhood': df_gmaps['neighborhood-political'].dropna().unique()\n",
        "})\n",
        "\n",
        "# Sort for readability\n",
        "df_neighborhood = df_neighborhood.sort_values(by='neighborhood').reset_index(drop=True)\n",
        "\n",
        "# Display the first 10 rows\n",
        "print(df_neighborhood.head(10))\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "05azNfeN8tWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare coordinate data\n",
        "gmaps_coords = df_gmaps[['latitude', 'longitude']].values\n",
        "housing_coords = df_housing[['latitude', 'longitude']].values\n",
        "\n",
        "# Fit NearestNeighbors model on df_gmaps\n",
        "nn_model = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
        "nn_model.fit(gmaps_coords)\n",
        "\n",
        "# Find the index of the closest neighbor in df_gmaps for each row in df_housing\n",
        "distances, indices = nn_model.kneighbors(housing_coords)\n",
        "\n",
        "# Get the corresponding neighborhood values from df_gmaps\n",
        "matched_neighborhoods = df_gmaps.iloc[indices.flatten()]['neighborhood-political'].values\n",
        "\n",
        "# Add the matched neighborhoods to df_housing\n",
        "df_housing['neighborhood'] = matched_neighborhoods\n",
        "\n",
        "# Save to CSV\n",
        "df_housing.to_csv('df_housing_appended.csv', index=False)"
      ],
      "metadata": {
        "id": "nlv5RJKA8yLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing by 'neighborhood' and calculate the average of 'median_house_age'\n",
        "average_house_age = df_housing.groupby('neighborhood')['housing_median_age'].mean().reset_index()\n",
        "\n",
        "# Rename the column to match the target column name\n",
        "average_house_age.columns = ['neighborhood', 'median-house-age']\n",
        "\n",
        "# Merge the average values into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(average_house_age, on='neighborhood', how='left')\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "fAg88AEn82Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current PC time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Add 'event_time' column with current time for each row\n",
        "df_neighborhood['event_time'] = current_time\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "bnw1TFyp86JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert a number to a decade range string\n",
        "def convert_to_age_range(age):\n",
        "    if pd.isna(age):\n",
        "        return None  # or a default value like \"Unknown\"\n",
        "    lower = int(age) // 10 * 10\n",
        "    upper = lower + 9\n",
        "    return f\"{lower} to {upper} years\"\n",
        "\n",
        "# Apply the function to the 'median-house-age' column\n",
        "df_neighborhood['median-house-age'] = df_neighborhood['median-house-age'].apply(convert_to_age_range)\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "hwN6MSye8-F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing by 'neighborhood' and compute the average of 'total_households'\n",
        "average_total_households = df_housing.groupby('neighborhood')['households'].mean().reset_index()\n",
        "\n",
        "# Rename the column for merging\n",
        "average_total_households.columns = ['neighborhood', 'total-households']\n",
        "\n",
        "# Merge the average values into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(average_total_households, on='neighborhood', how='left')\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "-DCGG2bt-HTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average median_house_value per neighborhood, capped at 500000\n",
        "avg_median_house_value = (\n",
        "    df_housing.groupby('neighborhood')['median_house_value']\n",
        "    .mean()\n",
        "    .clip(upper=500000)  # Cap at 500,000\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Rename column to match the target in df_neighborhood\n",
        "avg_median_house_value.columns = ['neighborhood', 'median-house-value']\n",
        "\n",
        "# Merge this result into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(avg_median_house_value, on='neighborhood', how='left')\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "g23S53CQ-K3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the 'medium-house-value' column to 0 decimal places and convert to int\n",
        "df_neighborhood['median-house-value'] = df_neighborhood['median-house-value'].round(0).astype('Int64')\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "79UndH-L-OK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing by 'neighborhood' and compute the average of 'total-households'\n",
        "average_total_households = df_housing.groupby('neighborhood')['households'].mean().reset_index()\n",
        "\n",
        "# Rename the column for merging\n",
        "average_total_households.columns = ['neighborhood', 'total-households']\n",
        "\n",
        "# Merge the average values into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(average_total_households, on='neighborhood', how='left')\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "_1U434e4-Rta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute average bedrooms_per_household per neighborhood in df_housing\n",
        "avg_bedrooms_per_household = (\n",
        "    df_housing.groupby('neighborhood')['bedrooms_per_household']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Rename the column for clarity\n",
        "avg_bedrooms_per_household.columns = ['neighborhood', 'bedrooms-per-household']\n",
        "\n",
        "# Merge into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(avg_bedrooms_per_household, on='neighborhood', how='left')\n",
        "\n",
        "# Save the modified DataFrame\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "pa9bervB-WYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the column to the nearest integer\n",
        "df_neighborhood['bedrooms-per-household'] = df_neighborhood['bedrooms-per-household'].round(0).astype('Int64')\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "284X_cFV-ZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'ocean_proximity' column\n",
        "df_housing = pd.get_dummies(df_housing, columns=['ocean_proximity'])\n",
        "\n",
        "# Save the expanded DataFrame to a CSV file\n",
        "df_housing.to_csv('housing_ocean_proximity_encoded.csv', index=False)\n",
        "\n",
        "# Display the result\n",
        "print(df_housing.head())\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_housing.to_csv('df_housing.csv', index=False)"
      ],
      "metadata": {
        "id": "CKBgkc0i-dEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_<1H OCEAN']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "6qXumqz5-fyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_INLAND']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "_cOCGFNb-jrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_ISLAND']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "hrZYULlp-mm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_NEAR BAY']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "znm83jP7-qLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_NEAR OCEAN']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "NTDh9vXS-s5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_neighborhood.head(10)"
      ],
      "metadata": {
        "id": "eJsPKJlw-wT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neighborhoods of interest\n",
        "neighborhoods_of_interest = [\"Brooktree\", \"Fisherman's Wharf\", \"Los Osos\"]\n",
        "\n",
        "# Filter and display the matching rows\n",
        "filtered_df = df_neighborhood[df_neighborhood['neighborhood'].isin(neighborhoods_of_interest)]\n",
        "print(filtered_df)"
      ],
      "metadata": {
        "id": "fzMbMd7t-zBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------"
      ],
      "metadata": {
        "id": "Ju60rc-A-5KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data_object = s3_client.get_object(\n",
        "    Bucket=fraud_detection_bucket_name, Key=identity_file_key\n",
        ")\n",
        "transaction_data_object = s3_client.get_object(\n",
        "    Bucket=fraud_detection_bucket_name, Key=transaction_file_key\n",
        ")\n",
        "\n",
        "identity_data = pd.read_csv(io.BytesIO(identity_data_object[\"Body\"].read()))\n",
        "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object[\"Body\"].read()))"
      ],
      "metadata": {
        "id": "glUTxTFL-2rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "#XXXXXXXXXXXXXXXXXX\n",
        "#fraud_detection_bucket_name replaced with home_value_prediction\n",
        "\n",
        "home_value_prediction = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\"housing_gmaps_data_raw.csv\"\n",
        "    #\"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_identity.csv\"\n",
        ")\n",
        "transaction_file_key = (\"housing.csv\"\n",
        "    #\"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_transactions.csv\"\n",
        ")\n",
        "\n",
        "identity_data_object = s3_client.get_object(\n",
        "    Bucket=home_value_prediction, Key=identity_file_key\n",
        ")\n",
        "transaction_data_object = s3_client.get_object(\n",
        "    Bucket=home_value_prediction, Key=transaction_file_key\n",
        ")\n",
        "\n",
        "identity_data = pd.read_csv(io.BytesIO(identity_data_object[\"Body\"].read()))\n",
        "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object[\"Body\"].read()))\n"
      ],
      "metadata": {
        "id": "k6ehpjdW--FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data.head()"
      ],
      "metadata": {
        "id": "bE6id_e__Ol2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data.head()"
      ],
      "metadata": {
        "id": "xoWmiGls_QEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OreKdVaH_TGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}