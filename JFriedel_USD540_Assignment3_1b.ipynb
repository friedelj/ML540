{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ0Mi4CxnMeAVpF82uedYW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/ML540/blob/main/JFriedel_USD540_Assignment3_1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JFriedel                                                     Assignment 3_1                                                     6-2-25"
      ],
      "metadata": {
        "id": "p3C4foJ76yT9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CRaaEQl6vEM"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "original_boto3_version = boto3.__version__\n",
        "%pip install 'boto3>1.17.21'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.session import Session\n",
        "\n",
        "region = boto3.Session().region_name\n",
        "\n",
        "boto_session = boto3.Session(region_name=region)\n",
        "\n",
        "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
        "featurestore_runtime = boto_session.client(\n",
        "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
        ")\n",
        "\n",
        "feature_store_session = Session(\n",
        "    boto_session=boto_session,\n",
        "    sagemaker_client=sagemaker_client,\n",
        "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
        ")"
      ],
      "metadata": {
        "id": "-ablpC5x65mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the following to use a bucket of your choosing\n",
        "default_s3_bucket_name = feature_store_session.default_bucket()\n",
        "prefix = \"sagemaker-featurestore-asmt3\"\n",
        "\n",
        "print(default_s3_bucket_name)"
      ],
      "metadata": {
        "id": "Z1KNtEQs6-rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "\n",
        "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
        "role = get_execution_role()\n",
        "print(role)"
      ],
      "metadata": {
        "id": "gmJNW_am7ByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "fraud_detection_bucket_name = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\"housing_gmaps_data_raw.csv\")\n",
        "transaction_file_key = (\"housing.csv\")"
      ],
      "metadata": {
        "id": "bBIUNWWm7FZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------DATA CLEANING"
      ],
      "metadata": {
        "id": "7qKl_3jx7Mpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import re\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "xi1KW9NT7IVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df_housing = pd.read_csv('housing.csv')\n",
        "\n",
        "# Load the CSV file\n",
        "df_gmaps = pd.read_csv('housing_gmaps_data_raw.csv')"
      ],
      "metadata": {
        "id": "y34TwZWa7X2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge df_housing with df_gmaps on 'longitude' and 'latitude'\n",
        "df_housing = df_housing.merge(\n",
        "    df_gmaps[['longitude', 'latitude', 'postal_code']],\n",
        "    on=['longitude', 'latitude'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Save the updated df_housing to a CSV file\n",
        "df_housing.to_csv('housing_with_postal_code.csv', index=False)"
      ],
      "metadata": {
        "id": "oohiLnne7bZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "grbA_kdv7eJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the rounded average total_bedrooms for each postal_code\n",
        "avg_bedrooms_by_postal = df_housing.groupby('postal_code')['total_bedrooms'].mean().round()\n",
        "\n",
        "# Define a function to apply the group average to missing values\n",
        "def fill_bedrooms(row):\n",
        "    if pd.isna(row['total_bedrooms']):\n",
        "        return avg_bedrooms_by_postal.get(row['postal_code'], np.nan)\n",
        "    else:\n",
        "        return row['total_bedrooms']\n",
        "\n",
        "# Apply the function\n",
        "df_housing['total_bedrooms'] = df_housing.apply(fill_bedrooms, axis=1)\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_housing.to_csv('housing_bedrooms_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "4eHdJupi7hib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "9ezaFQmg7lSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate known and unknown postal_code rows\n",
        "known = df_housing[df_housing['postal_code'].notna()].copy()\n",
        "unknown = df_housing[df_housing['postal_code'].isna()].copy()\n",
        "\n",
        "# Build a KDTree from known locations\n",
        "tree = cKDTree(known[['latitude', 'longitude']])\n",
        "\n",
        "# Query the closest known point for each unknown point\n",
        "distances, indices = tree.query(unknown[['latitude', 'longitude']], k=1)\n",
        "\n",
        "# Assign the closest known postal_code to the unknown rows\n",
        "closest_postal_codes = known.iloc[indices]['postal_code'].values\n",
        "df_housing.loc[unknown.index, 'postal_code'] = closest_postal_codes\n",
        "\n",
        "# Save the updated DataFrame to CSV\n",
        "df_housing.to_csv('housing_postal_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "7Y_Wmks77oTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "ctZjbSma7sNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new column by dividing total_bedrooms by households and rounding the result\n",
        "df_housing['bedrooms_per_household'] = (df_housing['total_bedrooms'] / df_housing['households']).round()\n",
        "\n",
        "# Save the updated DataFrame to CSV\n",
        "df_housing.to_csv('housing_bedrooms_per_household.csv', index=False)"
      ],
      "metadata": {
        "id": "O5O7jbRc7vV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing.head(10)"
      ],
      "metadata": {
        "id": "Zcdu6iMP7zOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_housing.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_housing.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "Ebaw_Ziw72Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_gmaps.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_gmaps.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "zqCirg4C76et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if needed)\n",
        "df_gmaps['administrative_area_level_1-political'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values with 'California'\n",
        "df_gmaps['administrative_area_level_1-political'].fillna('California', inplace=True)\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "w_CfV3677-E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if needed)\n",
        "df_gmaps['postal_code_suffix'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values with 9999 and convert the column to integer\n",
        "df_gmaps['postal_code_suffix'] = df_gmaps['postal_code_suffix'].fillna(9999).astype(int)\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "WffcrHDR8DsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if needed)\n",
        "df_gmaps['street_number'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values with \"0\" (as a string to match text-based route values)\n",
        "df_gmaps['street_number'] = df_gmaps['street_number'].fillna('0')\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "_3FV5rmx8HPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate rows with and without the target value\n",
        "df_missing = df_gmaps[df_gmaps['administrative_area_level_2-political'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['administrative_area_level_2-political'].notna()]\n",
        "\n",
        "# Fit NearestNeighbors on known locations\n",
        "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
        "nn.fit(df_known[['latitude', 'longitude']])\n",
        "\n",
        "# Find closest matches for missing rows\n",
        "distances, indices = nn.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill missing values\n",
        "df_gmaps.loc[df_missing.index, 'administrative_area_level_2-political'] = \\\n",
        "    df_known.iloc[indices.flatten()]['administrative_area_level_2-political'].values\n",
        "\n",
        "# Save the modified DataFrame to a CSV\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "OR8ByxU28KxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the route from the address\n",
        "def extract_route(address):\n",
        "    if pd.isna(address):\n",
        "        return None\n",
        "    match = re.search(r'\\d+\\s+([^,]+)', address)\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "# Replace empty strings with NaN if necessary\n",
        "df_gmaps['route'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing 'route' values\n",
        "df_gmaps['route'] = df_gmaps.apply(\n",
        "    lambda row: extract_route(row['address']) if pd.isna(row['route']) else row['route'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "ZcDPe0It8OV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty strings with NaN (if applicable)\n",
        "df_gmaps['route'].replace('', pd.NA, inplace=True)\n",
        "\n",
        "# Fill missing values in 'route' with the default string\n",
        "df_gmaps['route'] = df_gmaps['route'].fillna('strret name not filled in')\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "sEfUx-zt8SE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert empty strings to NaN\n",
        "df_gmaps['locality-political'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Identify rows with missing and non-missing 'locality-political'\n",
        "df_missing = df_gmaps[df_gmaps['locality-political'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['locality-political'].notna()]\n",
        "\n",
        "# Use NearestNeighbors to find closest known point\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df_known[['latitude', 'longitude']])\n",
        "distances, indices = nbrs.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill missing 'locality-political' with nearest neighbor's value\n",
        "for i, idx in enumerate(df_missing.index):\n",
        "    nearest_index = df_known.index[indices[i][0]]\n",
        "    df_gmaps.at[idx, 'locality-political'] = df_known.at[nearest_index, 'locality-political']\n",
        "\n",
        "# Save the modified DataFrame\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "Dv97p3c78VQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert empty strings to NaN\n",
        "df_gmaps['postal_code'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Split data into rows with missing and non-missing postal_code\n",
        "df_missing = df_gmaps[df_gmaps['postal_code'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['postal_code'].notna()]\n",
        "\n",
        "# Fit NearestNeighbors using non-missing data\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df_known[['latitude', 'longitude']])\n",
        "distances, indices = nbrs.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill missing postal_code values from nearest neighbor\n",
        "for i, idx in enumerate(df_missing.index):\n",
        "    nearest_idx = df_known.index[indices[i][0]]\n",
        "    df_gmaps.at[idx, 'postal_code'] = df_known.at[nearest_idx, 'postal_code']\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "_Ngk_o9c8Y1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert empty strings to NaN\n",
        "df_gmaps['neighborhood-political'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Split the DataFrame into rows with and without missing 'neighborhood-political'\n",
        "df_missing = df_gmaps[df_gmaps['neighborhood-political'].isna()]\n",
        "df_known = df_gmaps[df_gmaps['neighborhood-political'].notna()]\n",
        "\n",
        "# Fit NearestNeighbors model on known values\n",
        "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df_known[['latitude', 'longitude']])\n",
        "distances, indices = nbrs.kneighbors(df_missing[['latitude', 'longitude']])\n",
        "\n",
        "# Fill in missing 'neighborhood-political' values\n",
        "for i, idx in enumerate(df_missing.index):\n",
        "    nearest_idx = df_known.index[indices[i][0]]\n",
        "    df_gmaps.at[idx, 'neighborhood-political'] = df_known.at[nearest_idx, 'neighborhood-political']\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_gmaps.to_csv('df_gmaps_filled.csv', index=False)"
      ],
      "metadata": {
        "id": "GijXEh3_8c5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_gmaps.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_gmaps.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "aCWdn3Zj8gDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the first 12 columns\n",
        "df_gmaps = df_gmaps.iloc[:, :12]\n",
        "\n",
        "# Save the result to a new CSV\n",
        "df_gmaps.to_csv('housing_gmaps_data_trimmed.csv', index=False)\n",
        "\n",
        "# Display the result\n",
        "print(df_gmaps.head())"
      ],
      "metadata": {
        "id": "BXYC72aq8jiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display total missing values per column\n",
        "missing_per_column = df_gmaps.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_per_column)\n",
        "\n",
        "# Check if any value is missing in the entire DataFrame\n",
        "any_missing = df_gmaps.isnull().values.any()\n",
        "print(\"\\nIs there any missing data in the file?:\", any_missing)"
      ],
      "metadata": {
        "id": "o0t1icKk8nHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gmaps.head(10)"
      ],
      "metadata": {
        "id": "0bos1w608qas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract unique values (including NaNs if needed) and create the new DataFrame\n",
        "df_neighborhood = pd.DataFrame({\n",
        "    'neighborhood': df_gmaps['neighborhood-political'].dropna().unique()\n",
        "})\n",
        "\n",
        "# Sort for readability\n",
        "df_neighborhood = df_neighborhood.sort_values(by='neighborhood').reset_index(drop=True)\n",
        "\n",
        "# Display the first 10 rows\n",
        "print(df_neighborhood.head(10))\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "05azNfeN8tWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare coordinate data\n",
        "gmaps_coords = df_gmaps[['latitude', 'longitude']].values\n",
        "housing_coords = df_housing[['latitude', 'longitude']].values\n",
        "\n",
        "# Fit NearestNeighbors model on df_gmaps\n",
        "nn_model = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
        "nn_model.fit(gmaps_coords)\n",
        "\n",
        "# Find the index of the closest neighbor in df_gmaps for each row in df_housing\n",
        "distances, indices = nn_model.kneighbors(housing_coords)\n",
        "\n",
        "# Get the corresponding neighborhood values from df_gmaps\n",
        "matched_neighborhoods = df_gmaps.iloc[indices.flatten()]['neighborhood-political'].values\n",
        "\n",
        "# Add the matched neighborhoods to df_housing\n",
        "df_housing['neighborhood'] = matched_neighborhoods\n",
        "\n",
        "# Save to CSV\n",
        "df_housing.to_csv('df_housing_appended.csv', index=False)"
      ],
      "metadata": {
        "id": "nlv5RJKA8yLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing by 'neighborhood' and calculate the average of 'median_house_age'\n",
        "average_house_age = df_housing.groupby('neighborhood')['housing_median_age'].mean().reset_index()\n",
        "\n",
        "# Rename the column to match the target column name\n",
        "average_house_age.columns = ['neighborhood', 'median-house-age']\n",
        "\n",
        "# Merge the average values into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(average_house_age, on='neighborhood', how='left')\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "fAg88AEn82Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current PC time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Add 'event_time' column with current time for each row\n",
        "df_neighborhood['event_time'] = current_time\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "bnw1TFyp86JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert a number to a decade range string\n",
        "def convert_to_age_range(age):\n",
        "    if pd.isna(age):\n",
        "        return None  # or a default value like \"Unknown\"\n",
        "    lower = int(age) // 10 * 10\n",
        "    upper = lower + 9\n",
        "    return f\"{lower} to {upper} years\"\n",
        "\n",
        "# Apply the function to the 'median-house-age' column\n",
        "df_neighborhood['median-house-age'] = df_neighborhood['median-house-age'].apply(convert_to_age_range)\n",
        "\n",
        "# Save the modified DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "hwN6MSye8-F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing by 'neighborhood' and compute the average of 'total_households'\n",
        "average_total_households = df_housing.groupby('neighborhood')['households'].mean().reset_index()\n",
        "\n",
        "# Rename the column for merging\n",
        "average_total_households.columns = ['neighborhood', 'total-households']\n",
        "\n",
        "# Merge the average values into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(average_total_households, on='neighborhood', how='left')\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "-DCGG2bt-HTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average median_house_value per neighborhood, capped at 500000\n",
        "avg_median_house_value = (\n",
        "    df_housing.groupby('neighborhood')['median_house_value']\n",
        "    .mean()\n",
        "    .clip(upper=500000)  # Cap at 500,000\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Rename column to match the target in df_neighborhood\n",
        "avg_median_house_value.columns = ['neighborhood', 'median-house-value']\n",
        "\n",
        "# Merge this result into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(avg_median_house_value, on='neighborhood', how='left')\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "g23S53CQ-K3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the 'medium-house-value' column to 0 decimal places and convert to int\n",
        "df_neighborhood['median-house-value'] = df_neighborhood['median-house-value'].round(0).astype('Int64')\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "79UndH-L-OK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing by 'neighborhood' and compute the average of 'total-households'\n",
        "average_total_households = df_housing.groupby('neighborhood')['households'].mean().reset_index()\n",
        "\n",
        "# Rename the column for merging\n",
        "average_total_households.columns = ['neighborhood', 'total-households']\n",
        "\n",
        "# Merge the average values into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(average_total_households, on='neighborhood', how='left')\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "_1U434e4-Rta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute average bedrooms_per_household per neighborhood in df_housing\n",
        "avg_bedrooms_per_household = (\n",
        "    df_housing.groupby('neighborhood')['bedrooms_per_household']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Rename the column for clarity\n",
        "avg_bedrooms_per_household.columns = ['neighborhood', 'bedrooms-per-household']\n",
        "\n",
        "# Merge into df_neighborhood\n",
        "df_neighborhood = df_neighborhood.merge(avg_bedrooms_per_household, on='neighborhood', how='left')\n",
        "\n",
        "# Save the modified DataFrame\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "pa9bervB-WYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the column to the nearest integer\n",
        "df_neighborhood['bedrooms-per-household'] = df_neighborhood['bedrooms-per-household'].round(0).astype('Int64')\n",
        "\n",
        "# Save the modified DataFrame to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "284X_cFV-ZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'ocean_proximity' column\n",
        "df_housing = pd.get_dummies(df_housing, columns=['ocean_proximity'])\n",
        "\n",
        "# Save the expanded DataFrame to a CSV file\n",
        "df_housing.to_csv('housing_ocean_proximity_encoded.csv', index=False)\n",
        "\n",
        "# Display the result\n",
        "print(df_housing.head())\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_housing.to_csv('df_housing.csv', index=False)"
      ],
      "metadata": {
        "id": "CKBgkc0i-dEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_<1H OCEAN']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "6qXumqz5-fyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_INLAND']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "_cOCGFNb-jrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_ISLAND']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "hrZYULlp-mm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_NEAR BAY']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "znm83jP7-qLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df_housing to ensure one row per neighborhood, taking the first occurrence or mode\n",
        "df_ocean = df_housing[['neighborhood', 'ocean_proximity_NEAR OCEAN']].dropna().drop_duplicates(subset='neighborhood')\n",
        "\n",
        "# Merge with df_neighborhood on the 'neighborhood' column\n",
        "df_neighborhood = df_neighborhood.merge(df_ocean, on='neighborhood', how='left')\n",
        "\n",
        "# Save to CSV\n",
        "df_neighborhood.to_csv('df_neighborhood.csv', index=False)"
      ],
      "metadata": {
        "id": "NTDh9vXS-s5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_neighborhood.head(10)"
      ],
      "metadata": {
        "id": "eJsPKJlw-wT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neighborhoods of interest\n",
        "neighborhoods_of_interest = [\"Brooktree\", \"Fisherman's Wharf\", \"Los Osos\"]\n",
        "\n",
        "# Filter and display the matching rows\n",
        "filtered_df = df_neighborhood[df_neighborhood['neighborhood'].isin(neighborhoods_of_interest)]\n",
        "print(filtered_df)"
      ],
      "metadata": {
        "id": "fzMbMd7t-zBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------"
      ],
      "metadata": {
        "id": "Ju60rc-A-5KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup SageMaker FeatureStore"
      ],
      "metadata": {
        "id": "d9A33rfxI62U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "original_boto3_version = boto3.__version__\n",
        "%pip install 'boto3>1.17.21'"
      ],
      "metadata": {
        "id": "glUTxTFL-2rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "#XXXXXXXXXXXXXXXXXX\n",
        "#fraud_detection_bucket_name replaced with home_value_prediction\n",
        "\n",
        "home_value_prediction = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\"housing_gmaps_data_raw.csv\"\n",
        "    #\"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_identity.csv\"\n",
        ")\n",
        "transaction_file_key = (\"housing.csv\"\n",
        "    #\"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_transactions.csv\"\n",
        ")\n",
        "\n",
        "identity_data_object = s3_client.get_object(\n",
        "    Bucket=home_value_prediction, Key=identity_file_key\n",
        ")\n",
        "transaction_data_object = s3_client.get_object(\n",
        "    Bucket=home_value_prediction, Key=transaction_file_key\n",
        ")\n",
        "\n",
        "identity_data = pd.read_csv(io.BytesIO(identity_data_object[\"Body\"].read()))\n",
        "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object[\"Body\"].read()))\n"
      ],
      "metadata": {
        "id": "k6ehpjdW--FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_from sagemaker.session import Session\n",
        "\n",
        "region = boto3.Session().region_name\n",
        "\n",
        "boto_session = boto3.Session(region_name=region)\n",
        "\n",
        "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
        "featurestore_runtime = boto_session.client(\n",
        "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
        ")\n",
        "\n",
        "feature_store_session = Session(\n",
        "    boto_session=boto_session,\n",
        "    sagemaker_client=sagemaker_client,\n",
        "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
        ")data.head()"
      ],
      "metadata": {
        "id": "bE6id_e__Ol2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S3 Bucket Setup For The OfflineStore"
      ],
      "metadata": {
        "id": "CO4L59zzJRj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the following to use a bucket of your choosing\n",
        "default_s3_bucket_name = feature_store_session.default_bucket()\n",
        "prefix = \"sagemaker-featurestore-demo\"\n",
        "\n",
        "print(default_s3_bucket_name)"
      ],
      "metadata": {
        "id": "xoWmiGls_QEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "\n",
        "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
        "role = get_execution_role()\n",
        "print(role)"
      ],
      "metadata": {
        "id": "OreKdVaH_TGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "s3_client = boto3.client(\"s3\", region_name=region)"
      ],
      "metadata": {
        "id": "otf0GG3NJj3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_detection_bucket_name = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\"housing_gmaps_data_cleaned.csv\")\n",
        "transaction_file_key = (\"housing_cleaned.csv\")"
      ],
      "metadata": {
        "id": "xV8kgzNeJod-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = s3_client.list_objects_v2(Bucket=\"fraud-detection-bucket-name\")\n",
        "for obj in response.get(\"Contents\", []):\n",
        "    print(obj[\"Key\"])"
      ],
      "metadata": {
        "id": "nqqbxSC2JsfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data_object = s3_client.get_object(\n",
        "    Bucket=fraud_detection_bucket_name, Key=identity_file_key\n",
        ")\n",
        "transaction_data_object = s3_client.get_object(\n",
        "    Bucket=fraud_detection_bucket_name, Key=transaction_file_key\n",
        ")\n",
        "\n",
        "identity_data = pd.read_csv(io.BytesIO(identity_data_object[\"Body\"].read()))\n",
        "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object[\"Body\"].read()))"
      ],
      "metadata": {
        "id": "kBcqcpgPJzvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get current working directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "f0Md6pn6J7m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files and directories in the current directory\n",
        "files = os.listdir()\n",
        "print(\"Files and directories:\", files)"
      ],
      "metadata": {
        "id": "Jg9pzeSvJ-Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data.head()"
      ],
      "metadata": {
        "id": "R-m4sBVmKCDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data.head()"
      ],
      "metadata": {
        "id": "HflCkSBaKHFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "home_value_prediction = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\"housing_gmaps_data_raw.csv\"\n",
        ")\n",
        "transaction_file_key = (\"housing.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "L7LIFUNtKMXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "xfOIblrsKQJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data = identity_data.round(5)\n",
        "transaction_data = transaction_data.round(5)\n",
        "\n",
        "identity_data = identity_data.fillna(0)\n",
        "transaction_data = transaction_data.fillna(0)\n",
        "\n",
        "# Feature transformations for this dataset are applied before ingestion into FeatureStore.\n",
        "# One hot encode card4, card6\n",
        "encoded_house_value = pd.get_dummies(transaction_data[\"total_rooms\"], prefix=\"house_value\")\n",
        "encoded_postal_code = pd.get_dummies(transaction_data[\"total_bedrooms\"], prefix=\"postal_code\")\n",
        "\n",
        "transformed_transaction_data = pd.concat(\n",
        "    [transaction_data, encoded_house_value, encoded_postal_code], axis=1\n",
        ")\n",
        "# blank space is not allowed in feature name\n",
        "transformed_transaction_data = transformed_transaction_data.rename(\n",
        "    columns={\"neighborhood\": \"neighborhood\"}"
      ],
      "metadata": {
        "id": "UpIBBrVOKUAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data.head()"
      ],
      "metadata": {
        "id": "dcdQ4J5qKYi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_transaction_data.head()"
      ],
      "metadata": {
        "id": "4XDRNoC1Kbwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest Data into FeatureStore"
      ],
      "metadata": {
        "id": "YDAuaoJ8KiE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#      Define FeatureGroups"
      ],
      "metadata": {
        "id": "7zvODILNKnAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import gmtime, strftime, sleep\n",
        "\n",
        "identity_feature_group_name = \"identity-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
        "transaction_feature_group_name = \"transaction-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())"
      ],
      "metadata": {
        "id": "XmIyzkclKrBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.feature_store.feature_group import FeatureGroup\n",
        "\n",
        "identity_feature_group = FeatureGroup(\n",
        "    name=identity_feature_group_name, sagemaker_session=feature_store_session\n",
        ")\n",
        "transaction_feature_group = FeatureGroup(\n",
        "    name=transaction_feature_group_name, sagemaker_session=feature_store_session\n",
        ")"
      ],
      "metadata": {
        "id": "zzzvaKqcKv33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "current_time_sec = int(round(time.time()))\n",
        "\n",
        "bool_columns = transformed_transaction_data.select_dtypes(include='bool').columns\n",
        "transformed_transaction_data[bool_columns] = transformed_transaction_data[bool_columns].astype(int)\n",
        "\n",
        "\n",
        "def cast_object_to_string(data_frame):\n",
        "    for label in data_frame.columns:\n",
        "        if data_frame.dtypes[label] == \"object\":\n",
        "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
        "\n",
        "\n",
        "# cast object dtype to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type.\n",
        "cast_object_to_string(identity_data)\n",
        "cast_object_to_string(transformed_transaction_data)\n",
        "\n",
        "# record identifier and event time feature names\n",
        "record_identifier_feature_name = \"TransactionID\"\n",
        "event_time_feature_name = \"EventTime\"\n",
        "\n",
        "# append EventTime feature\n",
        "identity_data[event_time_feature_name] = pd.Series(\n",
        "    [current_time_sec] * len(identity_data), dtype=\"float64\"\n",
        ")\n",
        "transformed_transaction_data[event_time_feature_name] = pd.Series(\n",
        "    [current_time_sec] * len(transaction_data), dtype=\"float64\"\n",
        ")\n",
        "\n",
        "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
        "identity_feature_group.load_feature_definitions(data_frame=identity_data)\n",
        "# output is suppressed\n",
        "transaction_feature_group.load_feature_definitions(data_frame=transformed_transaction_data)\n",
        "# output is suppressed"
      ],
      "metadata": {
        "id": "Qkhh3-HMKy5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#                 Create FeatureGroups in SageMaker FeatureStore"
      ],
      "metadata": {
        "id": "VunlalH7K50t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_feature_group_creation_complete(feature_group):\n",
        "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
        "    while status == \"Creating\":\n",
        "        print(\"Waiting for Feature Group Creation\")\n",
        "        time.sleep(5)\n",
        "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
        "    if status != \"Created\":\n",
        "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
        "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
        "\n",
        "\n",
        "identity_feature_group.create(\n",
        "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
        "    record_identifier_name=record_identifier_feature_name,\n",
        "    event_time_feature_name=event_time_feature_name,\n",
        "    role_arn=role,\n",
        "    enable_online_store=True,\n",
        ")\n",
        "\n",
        "transaction_feature_group.create(\n",
        "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
        "    record_identifier_name=record_identifier_feature_name,\n",
        "    event_time_feature_name=event_time_feature_name,\n",
        "    role_arn=role,\n",
        "    enable_online_store=True,\n",
        ")\n",
        "\n",
        "wait_for_feature_group_creation_complete(feature_group=identity_feature_group)\n",
        "wait_for_feature_group_creation_complete(feature_group=transaction_feature_group)"
      ],
      "metadata": {
        "id": "x8ygLEoTK4dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#     Confirm the FeatureGroup has been created by using the DescribeFeatureGroup and ListFeatureGroups APIs  "
      ],
      "metadata": {
        "id": "j9DSleYfLF9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identity_feature_group.describe()"
      ],
      "metadata": {
        "id": "FT0EmC-iLJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_feature_group.describe()"
      ],
      "metadata": {
        "id": "RZjrm8IxLOUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagemaker_client.list_feature_groups()  # use boto client to list FeatureGroups"
      ],
      "metadata": {
        "id": "UlqMcZnQLTCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#       PutRecords into FeatureGroup   "
      ],
      "metadata": {
        "id": "BaWeJgGDLWx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identity_feature_group.ingest(data_frame=identity_data, max_workers=3, wait=True)"
      ],
      "metadata": {
        "id": "iG49OcglLcuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_feature_group.ingest(data_frame=transformed_transaction_data, max_workers=5, wait=True)"
      ],
      "metadata": {
        "id": "4D-ZW0sWLgiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          To confirm that data has been ingested, we can quickly retrieve a record from the online store:"
      ],
      "metadata": {
        "id": "szLAn9QdLkjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_identifier_value = str(2990130)\n",
        "\n",
        "featurestore_runtime.get_record(\n",
        "    FeatureGroupName=transaction_feature_group_name,\n",
        "    RecordIdentifierValueAsString=record_identifier_value,\n",
        ")"
      ],
      "metadata": {
        "id": "2N60rYmaLor3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featurestore_runtime.batch_get_record(\n",
        "    Identifiers=[\n",
        "        {\n",
        "            \"FeatureGroupName\": identity_feature_group_name,\n",
        "            \"RecordIdentifiersValueAsString\": [\"2990130\"],\n",
        "        },\n",
        "        {\n",
        "            \"FeatureGroupName\": transaction_feature_group_name,\n",
        "            \"RecordIdentifiersValueAsString\": [\"2990130\"],\n",
        "        },\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "g8yd2kD9LtsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##     The SageMaker Python SDK’s FeatureStore class also provides the functionality to generate Hive DDL commands.\n",
        "##     Schema of the table is generated based on the feature definitions.\n",
        "##     Columns are named after feature name and data-type are inferred based on feature type."
      ],
      "metadata": {
        "id": "G9pQo3RSLyKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(identity_feature_group.as_hive_ddl())"
      ],
      "metadata": {
        "id": "neH9fm68L2o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
        "print(account_id)\n",
        "\n",
        "identity_feature_group_resolved_output_s3_uri = (\n",
        "    identity_feature_group.describe()\n",
        "    .get(\"OfflineStoreConfig\")\n",
        "    .get(\"S3StorageConfig\")\n",
        "    .get(\"ResolvedOutputS3Uri\")\n",
        ")\n",
        "\n",
        "transaction_feature_group_resolved_output_s3_uri = (\n",
        "    transaction_feature_group.describe()\n",
        "    .get(\"OfflineStoreConfig\")\n",
        "    .get(\"S3StorageConfig\")\n",
        "    .get(\"ResolvedOutputS3Uri\")\n",
        ")\n",
        "\n",
        "identity_feature_group = FeatureGroup(name=\"your-identity-feature-group-name\", sagemaker_session=sagemaker_session)\n",
        "\n",
        "\n",
        "transaction_feature_group_s3_prefix = transaction_feature_group_resolved_output_s3_uri.replace(\n",
        "    f\"s3://{default_s3_bucket_name}/\", \"\"\n",
        ")\n",
        "\n",
        "offline_store_contents = None\n",
        "while offline_store_contents is None:\n",
        "    objects_in_bucket = s3_client.list_objects(\n",
        "        Bucket=default_s3_bucket_name, Prefix=transaction_feature_group_s3_prefix\n",
        "    )\n",
        "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
        "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
        "    else:\n",
        "        print(\"Waiting for data in offline store...\\n\")\n",
        "        sleep(60)\n",
        "\n",
        "print(\"Data available.\")"
      ],
      "metadata": {
        "id": "Of-5D3jPL6Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XXXXXXXXXXXXXXXXXXXXXXXXx\n",
        "from sagemaker import Session\n",
        "\n",
        "sagemaker_session = Session()\n",
        "client = sagemaker_session.sagemaker_client\n",
        "\n",
        "response = client.list_feature_groups()\n",
        "for fg in response[\"FeatureGroupSummaries\"]:\n",
        "    print(fg[\"FeatureGroupName\"])"
      ],
      "metadata": {
        "id": "SgFAje6tMKmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transaction_feature_group.as_hive_ddl())"
      ],
      "metadata": {
        "id": "mWCDbNMNMOOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
        "print(account_id)\n",
        "\n",
        "identity_feature_group_resolved_output_s3_uri = (\n",
        "    identity_feature_group.describe()\n",
        "    .get(\"OfflineStoreConfig\")\n",
        "    .get(\"S3StorageConfig\")\n",
        "    .get(\"ResolvedOutputS3Uri\")\n",
        ")\n",
        "\n",
        "transaction_feature_group_resolved_output_s3_uri = (\n",
        "    transaction_feature_group.describe()\n",
        "    .get(\"OfflineStoreConfig\")\n",
        "    .get(\"S3StorageConfig\")\n",
        "    .get(\"ResolvedOutputS3Uri\")\n",
        ")\n",
        "\n",
        "identity_feature_group = FeatureGroup(name=\"your-identity-feature-group-name\", sagemaker_session=sagemaker_session)\n",
        "\n",
        "\n",
        "transaction_feature_group_s3_prefix = transaction_feature_group_resolved_output_s3_uri.replace(\n",
        "    f\"s3://{default_s3_bucket_name}/\", \"\"\n",
        ")\n",
        "\n",
        "offline_store_contents = None\n",
        "while offline_store_contents is None:\n",
        "    objects_in_bucket = s3_client.list_objects(\n",
        "        Bucket=default_s3_bucket_name, Prefix=transaction_feature_group_s3_prefix\n",
        "    )\n",
        "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
        "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
        "    else:\n",
        "        print(\"Waiting for data in offline store...\\n\")\n",
        "        sleep(60)\n",
        "\n",
        "print(\"Data available.\")"
      ],
      "metadata": {
        "id": "jJd57qseMRcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Training Dataset"
      ],
      "metadata": {
        "id": "5O8vZa9_MXSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XXXXXXXXXXXXXXXXXXXXXXXXXXx\n",
        "from sagemaker import Session\n",
        "\n",
        "sagemaker_session = Session()\n",
        "client = sagemaker_session.sagemaker_client\n",
        "\n",
        "response = client.list_feature_groups()\n",
        "\n",
        "print(\"Available Feature Groups:\")\n",
        "for fg in response[\"FeatureGroupSummaries\"]:\n",
        "    print(\"-\", fg[\"FeatureGroupName\"])"
      ],
      "metadata": {
        "id": "5klvP_9sMa2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_query = identity_feature_group.athena_query()\n",
        "transaction_query = transaction_feature_group.athena_query()\n",
        "\n",
        "identity_table = identity_query.table_name\n",
        "transaction_table = transaction_query.table_name\n",
        "\n",
        "query_string = (\n",
        "    'SELECT * FROM \"'\n",
        "    + transaction_table\n",
        "    + '\" LEFT JOIN \"'\n",
        "    + identity_table\n",
        "    + '\" ON \"'\n",
        "    + transaction_table\n",
        "    + '\".transactionid = \"'\n",
        "    + identity_table\n",
        "    + '\".transactionid'\n",
        ")\n",
        "print(\"Running \" + query_string)\n",
        "\n",
        "# run Athena query. The output is loaded to a Pandas dataframe.\n",
        "# dataset = pd.DataFrame()\n",
        "identity_query.run(\n",
        "    query_string=query_string,\n",
        "    output_location=\"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/query_results/\",\n",
        ")\n",
        "identity_query.wait()\n",
        "dataset = identity_query.as_dataframe()\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "3GbRSyW5Me7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.feature_store.feature_group import FeatureGroup\n",
        "\n",
        "# Create the session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Initialize the feature group object\n",
        "identity_feature_group = FeatureGroup(\n",
        "    name=\"your-identity-feature-group-name\",\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "query_string = f\"\"\"\n",
        "SELECT *\n",
        "FROM \"{identity_feature_group.name}\"\n",
        "\"\"\"\n",
        "\n",
        "output_location = f\"s3://{default_s3_bucket_name}/{prefix}/athena-results/\"\n",
        "\n",
        "identity_query = identity_feature_group.athena_query()\n",
        "identity_query.run(query_string=query_string, output_location=output_location)\n",
        "identity_query.wait()\n",
        "\n",
        "query_execution = identity_query.get_query_execution()\n",
        "query_result = (\n",
        "    \"s3://\"\n",
        "    + default_s3_bucket_name\n",
        "    + \"/\"\n",
        "    + prefix\n",
        "    + \"/query_results/\"\n",
        "    + query_execution[\"QueryExecution\"][\"QueryExecutionId\"]\n",
        "    + \".csv\"\n",
        ")\n",
        "print(query_result)\n",
        "\n",
        "# Select useful columns for training with target column as the first.\n",
        "dataset = dataset[\n",
        "    [\n",
        "        \"house_value\",\n",
        "        \"neighborhood\",\n",
        "        \"postal_code\",\n",
        "        \"number_of_rooms\",\n",
        "        \"number_of_bedrooms\",\n",
        "        \"ocean_proximity_<1H OCEAN\",\n",
        "        \"ocean_proximity_INLAND\",\n",
        "        \"ocean_proximity_ISLAND\",\n",
        "        \"ocean_proximity_NEAR BAY\",\n",
        "        \"ocean_proximity_NEAR OCEAN\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Write to csv in S3 without headers and index column.\n",
        "dataset.to_csv(\"dataset.csv\", header=False, index=False)\n",
        "s3_client.upload_file(\"dataset.csv\", default_s3_bucket_name, prefix + \"/training_input/dataset.csv\")\n",
        "dataset_uri_prefix = \"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/training_input/\"\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "TJCk3hzOMjtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Deploy the Model"
      ],
      "metadata": {
        "id": "GwH7IzaPM1kJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#     Construct a SageMaker generic estimator using the SageMaker XGBoost container"
      ],
      "metadata": {
        "id": "etvD3-sFM9p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_image = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.0-1\")"
      ],
      "metadata": {
        "id": "aLpeSmPsM4kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#     Construct a SageMaker generic estimator using the SageMaker XGBoost container"
      ],
      "metadata": {
        "id": "dWuMftlCNF_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_output_path = \"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/training_output\"\n",
        "\n",
        "from sagemaker.estimator import Estimator\n",
        "\n",
        "training_model = Estimator(\n",
        "    training_image,\n",
        "    role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    volume_size=5,\n",
        "    max_run=3600,\n",
        "    input_mode=\"File\",\n",
        "    output_path=training_output_path,\n",
        "    sagemaker_session=feature_store_session,\n",
        ")"
      ],
      "metadata": {
        "id": "rv541zq7NBS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#       Set hyperparameters"
      ],
      "metadata": {
        "id": "SzCOl0fbNWpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_model.set_hyperparameters(objective=\"binary:logistic\", num_round=50)"
      ],
      "metadata": {
        "id": "xebgj6oqNadW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#       Specify training dataset"
      ],
      "metadata": {
        "id": "JsIs2tVyNeX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sagemaker.inputs.TrainingInput(\n",
        "    dataset_uri_prefix,\n",
        "    distribution=\"FullyReplicated\",\n",
        "    content_type=\"text/csv\",\n",
        "    s3_data_type=\"S3Prefix\",\n",
        ")\n",
        "data_channels = {\"train\": train_data}"
      ],
      "metadata": {
        "id": "OC71Mul4NiW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start training"
      ],
      "metadata": {
        "id": "COFzAVhTNl_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_model.fit(inputs=data_channels, logs=True)"
      ],
      "metadata": {
        "id": "CogCSxJ9NqHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Hosting for the Model"
      ],
      "metadata": {
        "id": "_6CT935wNvc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = training_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
      ],
      "metadata": {
        "id": "xEKJQei8N7Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SageMaker FeatureStore During Inference"
      ],
      "metadata": {
        "id": "rWlil-NeOEzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incoming inference request.\n",
        "transaction_id = str(3450774)\n",
        "\n",
        "\n",
        "# Helper to parse the feature value from the record.\n",
        "def get_feature_value(record, feature_name):\n",
        "    return str(list(filter(lambda r: r[\"FeatureName\"] == feature_name, record))[0][\"ValueAsString\"])\n",
        "\n",
        "\n",
        "transaction_response = featurestore_runtime.get_record(\n",
        "    FeatureGroupName=transaction_feature_group_name, RecordIdentifierValueAsString=transaction_id\n",
        ")\n",
        "transaction_record = transaction_response[\"Record\"]\n",
        "\n",
        "transaction_test_data = [\n",
        "    get_feature_value(transaction_record, \"house_value\"),\n",
        "    get_feature_value(transaction_record, \"neighborhood\"),\n",
        "    get_feature_value(transaction_record, \"postal_code\"),\n",
        "    get_feature_value(transaction_record, \"number_of_rooms\"),\n",
        "    get_feature_value(transaction_record, \"number_of_bedrooms\"),\n",
        "    get_feature_value(transaction_record, \"ocean_proximity_<1H OCEAN\"),\n",
        "    get_feature_value(transaction_record, \"ocean_proximity_INLAND\"),\n",
        "    get_feature_value(transaction_record, \"ocean_proximity_ISLAND\"),\n",
        "    get_feature_value(transaction_record, \"ocean_proximity_NEAR BAY\"),\n",
        "    get_feature_value(transaction_record, \"ocean_proximity_NEAR OCEAN\"),\n",
        "]\n",
        "\n",
        "identity_response = featurestore_runtime.get_record(\n",
        "    FeatureGroupName=identity_feature_group_name, RecordIdentifierValueAsString=transaction_id\n",
        ")\n",
        "identity_record = identity_response[\"Record\"]\n",
        "id_test_data = [\n",
        "    get_feature_value(identity_record, \"id_01\"),\n",
        "    get_feature_value(identity_record, \"id_02\"),\n",
        "    get_feature_value(identity_record, \"id_03\"),\n",
        "    get_feature_value(identity_record, \"id_04\"),\n",
        "    get_feature_value(identity_record, \"id_05\"),\n",
        "]\n",
        "\n",
        "# Join all pieces for inference request.\n",
        "inference_request = []\n",
        "inference_request.extend(transaction_test_data[:])\n",
        "inference_request.extend(id_test_data[:])\n",
        "\n",
        "inference_request"
      ],
      "metadata": {
        "id": "iM_Bs4CYOPpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "results = predictor.predict(\",\".join(inference_request), initial_args={\"ContentType\": \"text/csv\"})\n",
        "prediction = json.loads(results)\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "L6QeupCzOjxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup Resources"
      ],
      "metadata": {
        "id": "SLai-_s3Oq7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.delete_endpoint()"
      ],
      "metadata": {
        "id": "SFbuXprzOuVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_feature_group.delete()\n",
        "transaction_feature_group.delete()"
      ],
      "metadata": {
        "id": "wtdT903AOx0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restore original boto3 version\n",
        "%pip install 'boto3=={}'.format(original_boto3_version)"
      ],
      "metadata": {
        "id": "-ZwmPBQkO1fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########EXERCISE"
      ],
      "metadata": {
        "id": "dbrgwPWRO49K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "original_boto3_version = boto3.__version__\n",
        "%pip install 'boto3>1.17.21"
      ],
      "metadata": {
        "id": "fpRw5mpYO8cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.session import Session\n",
        "\n",
        "region = boto3.Session().region_name\n",
        "\n",
        "boto_session = boto3.Session(region_name=region)\n",
        "\n",
        "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
        "featurestore_runtime = boto_session.client(\n",
        "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
        ")\n",
        "\n",
        "feature_store_session = Session(\n",
        "    boto_session=boto_session,\n",
        "    sagemaker_client=sagemaker_client,\n",
        "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
        ")"
      ],
      "metadata": {
        "id": "g6tC-3z4O__C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the following to use a bucket of your choosing\n",
        "default_s3_bucket_name = feature_store_session.default_bucket()\n",
        "prefix = \"sagemaker-featurestore-demo\"\n",
        "\n",
        "print(default_s3_bucket_name)"
      ],
      "metadata": {
        "id": "FaCVit3YPEkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "\n",
        "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
        "role = get_execution_role()\n",
        "print(role)"
      ],
      "metadata": {
        "id": "k1zblpFYPH2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "fraud_detection_bucket_name = f\"sagemaker-example-files-prod-{region}\"\n",
        "identity_file_key = (\n",
        "    \"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_identity.csv\"\n",
        ")\n",
        "transaction_file_key = (\n",
        "    \"datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_transactions.csv\"\n",
        ")\n",
        "\n",
        "identity_data_object = s3_client.get_object(\n",
        "    Bucket=fraud_detection_bucket_name, Key=identity_file_key\n",
        ")\n",
        "transaction_data_object = s3_client.get_object(\n",
        "    Bucket=fraud_detection_bucket_name, Key=transaction_file_key\n",
        ")\n",
        "\n",
        "identity_data = pd.read_csv(io.BytesIO(identity_data_object[\"Body\"].read()))\n",
        "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object[\"Body\"].read()))"
      ],
      "metadata": {
        "id": "yBkssneUPMWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data.head()"
      ],
      "metadata": {
        "id": "ed-4zD_HPRAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data.head()"
      ],
      "metadata": {
        "id": "PU51_YS9PVCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data = identity_data.round(5)\n",
        "transaction_data = transaction_data.round(5)\n",
        "\n",
        "identity_data = identity_data.fillna(0)\n",
        "transaction_data = transaction_data.fillna(0)\n",
        "\n",
        "# Feature transformations for this dataset are applied before ingestion into FeatureStore.\n",
        "# One hot encode card4, card6\n",
        "encoded_card_bank = pd.get_dummies(transaction_data[\"card4\"], prefix=\"card_bank\")\n",
        "encoded_card_type = pd.get_dummies(transaction_data[\"card6\"], prefix=\"card_type\")\n",
        "\n",
        "transformed_transaction_data = pd.concat(\n",
        "    [transaction_data, encoded_card_type, encoded_card_bank], axis=1\n",
        ")\n",
        "# blank space is not allowed in feature name\n",
        "transformed_transaction_data = transformed_transaction_data.rename(\n",
        "    columns={\"card_bank_american express\": \"card_bank_american_express\"}\n",
        ")"
      ],
      "metadata": {
        "id": "EMlXy7VYPaYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_data.head()"
      ],
      "metadata": {
        "id": "DfBEisSFPflr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_transaction_data.head()"
      ],
      "metadata": {
        "id": "wEXXREP5PiPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import gmtime, strftime, sleep\n",
        "\n",
        "identity_feature_group_name = \"identity-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
        "transaction_feature_group_name = \"transaction-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())"
      ],
      "metadata": {
        "id": "CxW4PYlxPmXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.feature_store.feature_group import FeatureGroup\n",
        "\n",
        "identity_feature_group = FeatureGroup(\n",
        "    name=identity_feature_group_name, sagemaker_session=feature_store_session\n",
        ")\n",
        "transaction_feature_group = FeatureGroup(\n",
        "    name=transaction_feature_group_name, sagemaker_session=feature_store_session\n",
        ")"
      ],
      "metadata": {
        "id": "h8eqTEYMPpNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "current_time_sec = int(round(time.time()))\n",
        "\n",
        "bool_columns = transformed_transaction_data.select_dtypes(include='bool').columns\n",
        "transformed_transaction_data[bool_columns] = transformed_transaction_data[bool_columns].astype(int)\n",
        "\n",
        "\n",
        "def cast_object_to_string(data_frame):\n",
        "    for label in data_frame.columns:\n",
        "        if data_frame.dtypes[label] == \"object\":\n",
        "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
        "\n",
        "\n",
        "# cast object dtype to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type.\n",
        "cast_object_to_string(identity_data)\n",
        "cast_object_to_string(transformed_transaction_data)\n",
        "\n",
        "# record identifier and event time feature names\n",
        "record_identifier_feature_name = \"TransactionID\"\n",
        "event_time_feature_name = \"EventTime\"\n",
        "\n",
        "# append EventTime feature\n",
        "identity_data[event_time_feature_name] = pd.Series(\n",
        "    [current_time_sec] * len(identity_data), dtype=\"float64\"\n",
        ")\n",
        "transformed_transaction_data[event_time_feature_name] = pd.Series(\n",
        "    [current_time_sec] * len(transaction_data), dtype=\"float64\"\n",
        ")\n",
        "\n",
        "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
        "identity_feature_group.load_feature_definitions(data_frame=identity_data)\n",
        "# output is suppressed\n",
        "transaction_feature_group.load_feature_definitions(data_frame=transformed_transaction_data)\n",
        "# output is suppressed"
      ],
      "metadata": {
        "id": "v_GDV8JSPtKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_feature_group_creation_complete(feature_group):\n",
        "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
        "    while status == \"Creating\":\n",
        "        print(\"Waiting for Feature Group Creation\")\n",
        "        time.sleep(5)\n",
        "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
        "    if status != \"Created\":\n",
        "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
        "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
        "\n",
        "\n",
        "identity_feature_group.create(\n",
        "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
        "    record_identifier_name=record_identifier_feature_name,\n",
        "    event_time_feature_name=event_time_feature_name,\n",
        "    role_arn=role,\n",
        "    enable_online_store=True,\n",
        ")\n",
        "\n",
        "transaction_feature_group.create(\n",
        "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
        "    record_identifier_name=record_identifier_feature_name,\n",
        "    event_time_feature_name=event_time_feature_name,\n",
        "    role_arn=role,\n",
        "    enable_online_store=True,\n",
        ")\n",
        "\n",
        "wait_for_feature_group_creation_complete(feature_group=identity_feature_group)\n",
        "wait_for_feature_group_creation_complete(feature_group=transaction_feature_group)"
      ],
      "metadata": {
        "id": "S91YKVNXP323"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_feature_group.describe()"
      ],
      "metadata": {
        "id": "wxBafbvmP5zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_feature_group.describe()"
      ],
      "metadata": {
        "id": "RcmpvYhrQHnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagemaker_client.list_feature_groups()  # use boto client to list FeatureGroups"
      ],
      "metadata": {
        "id": "BwNj1ol7QNxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_feature_group.ingest(data_frame=identity_data, max_workers=3, wait=True)"
      ],
      "metadata": {
        "id": "h2comh4_QTJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_feature_group.ingest(data_frame=transformed_transaction_data, max_workers=5, wait=True)"
      ],
      "metadata": {
        "id": "PUtotiDkQVXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_identifier_value = str(2990130)\n",
        "\n",
        "featurestore_runtime.get_record(\n",
        "    FeatureGroupName=transaction_feature_group_name,\n",
        "    RecordIdentifierValueAsString=record_identifier_value,\n",
        ")"
      ],
      "metadata": {
        "id": "-Icq2_ZqQchu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featurestore_runtime.batch_get_record(\n",
        "    Identifiers=[\n",
        "        {\n",
        "            \"FeatureGroupName\": identity_feature_group_name,\n",
        "            \"RecordIdentifiersValueAsString\": [\"2990130\"],\n",
        "        },\n",
        "        {\n",
        "            \"FeatureGroupName\": transaction_feature_group_name,\n",
        "            \"RecordIdentifiersValueAsString\": [\"2990130\"],\n",
        "        },\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0BDcrdbMQiYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(identity_feature_group.as_hive_ddl())"
      ],
      "metadata": {
        "id": "dHPRdejLQou7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
        "print(account_id)\n",
        "\n",
        "identity_feature_group_resolved_output_s3_uri = (\n",
        "    identity_feature_group.describe()\n",
        "    .get(\"OfflineStoreConfig\")\n",
        "    .get(\"S3StorageConfig\")\n",
        "    .get(\"ResolvedOutputS3Uri\")\n",
        ")\n",
        "\n",
        "transaction_feature_group_resolved_output_s3_uri = (\n",
        "    transaction_feature_group.describe()\n",
        "    .get(\"OfflineStoreConfig\")\n",
        "    .get(\"S3StorageConfig\")\n",
        "    .get(\"ResolvedOutputS3Uri\")\n",
        ")\n",
        "\n",
        "identity_feature_group = FeatureGroup(name=\"your-identity-feature-group-name\", sagemaker_session=sagemaker_session)\n",
        "\n",
        "\n",
        "transaction_feature_group_s3_prefix = transaction_feature_group_resolved_output_s3_uri.replace(\n",
        "    f\"s3://{default_s3_bucket_name}/\", \"\"\n",
        ")\n",
        "\n",
        "offline_store_contents = None\n",
        "while offline_store_contents is None:\n",
        "    objects_in_bucket = s3_client.list_objects(\n",
        "        Bucket=default_s3_bucket_name, Prefix=transaction_feature_group_s3_prefix\n",
        "    )\n",
        "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
        "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
        "    else:\n",
        "        print(\"Waiting for data in offline store...\\n\")\n",
        "        sleep(60)\n",
        "\n",
        "print(\"Data available.\")"
      ],
      "metadata": {
        "id": "0zGXlPJCQsvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XXXXXXXXXXXXXXXXXXXXXXXXXXx\n",
        "from sagemaker import Session\n",
        "\n",
        "sagemaker_session = Session()\n",
        "client = sagemaker_session.sagemaker_client\n",
        "\n",
        "response = client.list_feature_groups()\n",
        "\n",
        "print(\"Available Feature Groups:\")\n",
        "for fg in response[\"FeatureGroupSummaries\"]:\n",
        "    print(\"-\", fg[\"FeatureGroupName\"])"
      ],
      "metadata": {
        "id": "D6Sfs7-2Q07P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_query = identity_feature_group.athena_query()\n",
        "transaction_query = transaction_feature_group.athena_query()\n",
        "\n",
        "identity_table = identity_query.table_name\n",
        "transaction_table = transaction_query.table_name\n",
        "\n",
        "query_string = (\n",
        "    'SELECT * FROM \"'\n",
        "    + transaction_table\n",
        "    + '\" LEFT JOIN \"'\n",
        "    + identity_table\n",
        "    + '\" ON \"'\n",
        "    + transaction_table\n",
        "    + '\".transactionid = \"'\n",
        "    + identity_table\n",
        "    + '\".transactionid'\n",
        ")\n",
        "print(\"Running \" + query_string)\n",
        "\n",
        "# run Athena query. The output is loaded to a Pandas dataframe.\n",
        "# dataset = pd.DataFrame()\n",
        "identity_query.run(\n",
        "    query_string=query_string,\n",
        "    output_location=\"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/query_results/\",\n",
        ")\n",
        "identity_query.wait()\n",
        "dataset = identity_query.as_dataframe()\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "TTiEOX8IRDpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.feature_store.feature_group import FeatureGroup\n",
        "\n",
        "# Create the session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Initialize the feature group object\n",
        "identity_feature_group = FeatureGroup(\n",
        "    name=\"your-identity-feature-group-name\",\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "query_string = f\"\"\"\n",
        "SELECT *\n",
        "FROM \"{identity_feature_group.name}\"\n",
        "\"\"\"\n",
        "\n",
        "output_location = f\"s3://{default_s3_bucket_name}/{prefix}/athena-results/\"\n",
        "\n",
        "identity_query = identity_feature_group.athena_query()\n",
        "identity_query.run(query_string=query_string, output_location=output_location)\n",
        "identity_query.wait()\n",
        "\n",
        "query_execution = identity_query.get_query_execution()\n",
        "query_result = (\n",
        "    \"s3://\"\n",
        "    + default_s3_bucket_name\n",
        "    + \"/\"\n",
        "    + prefix\n",
        "    + \"/query_results/\"\n",
        "    + query_execution[\"QueryExecution\"][\"QueryExecutionId\"]\n",
        "    + \".csv\"\n",
        ")\n",
        "print(query_result)\n",
        "\n",
        "# Select useful columns for training with target column as the first.\n",
        "dataset = dataset[\n",
        "    [\n",
        "        \"isfraud\",\n",
        "        \"transactiondt\",\n",
        "        \"transactionamt\",\n",
        "        \"card1\",\n",
        "        \"card2\",\n",
        "        \"card3\",\n",
        "        \"card5\",\n",
        "        \"card_type_credit\",\n",
        "        \"card_type_debit\",\n",
        "        \"card_bank_american_express\",\n",
        "        \"card_bank_discover\",\n",
        "        \"card_bank_mastercard\",\n",
        "        \"card_bank_visa\",\n",
        "        \"id_01\",\n",
        "        \"id_02\",\n",
        "        \"id_03\",\n",
        "        \"id_04\",\n",
        "        \"id_05\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Write to csv in S3 without headers and index column.\n",
        "dataset.to_csv(\"dataset.csv\", header=False, index=False)\n",
        "s3_client.upload_file(\"dataset.csv\", default_s3_bucket_name, prefix + \"/training_input/dataset.csv\")\n",
        "dataset_uri_prefix = \"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/training_input/\"\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "I2Mx_StwRG-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_image = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.0-1\")"
      ],
      "metadata": {
        "id": "gwt_0i_wROLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_output_path = \"s3://\" + default_s3_bucket_name + \"/\" + prefix + \"/training_output\"\n",
        "\n",
        "from sagemaker.estimator import Estimator\n",
        "\n",
        "training_model = Estimator(\n",
        "    training_image,\n",
        "    role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    volume_size=5,\n",
        "    max_run=3600,\n",
        "    input_mode=\"File\",\n",
        "    output_path=training_output_path,\n",
        "    sagemaker_session=feature_store_session,\n",
        ")"
      ],
      "metadata": {
        "id": "5GLl6ixWRRQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_model.set_hyperparameters(objective=\"binary:logistic\", num_round=50)"
      ],
      "metadata": {
        "id": "wK8NwxUnRVfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sagemaker.inputs.TrainingInput(\n",
        "    dataset_uri_prefix,\n",
        "    distribution=\"FullyReplicated\",\n",
        "    content_type=\"text/csv\",\n",
        "    s3_data_type=\"S3Prefix\",\n",
        ")\n",
        "data_channels = {\"train\": train_data}"
      ],
      "metadata": {
        "id": "pvKcbHO5RZmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_model.fit(inputs=data_channels, logs=True)"
      ],
      "metadata": {
        "id": "shGLb9baRhoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = training_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
      ],
      "metadata": {
        "id": "rbLKY3JeRjtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Incoming inference request.\n",
        "transaction_id = str(3450774)\n",
        "\n",
        "\n",
        "# Helper to parse the feature value from the record.\n",
        "def get_feature_value(record, feature_name):\n",
        "    return str(list(filter(lambda r: r[\"FeatureName\"] == feature_name, record))[0][\"ValueAsString\"])\n",
        "\n",
        "\n",
        "transaction_response = featurestore_runtime.get_record(\n",
        "    FeatureGroupName=transaction_feature_group_name, RecordIdentifierValueAsString=transaction_id\n",
        ")\n",
        "transaction_record = transaction_response[\"Record\"]\n",
        "\n",
        "transaction_test_data = [\n",
        "    get_feature_value(transaction_record, \"TransactionDT\"),\n",
        "    get_feature_value(transaction_record, \"TransactionAmt\"),\n",
        "    get_feature_value(transaction_record, \"card1\"),\n",
        "    get_feature_value(transaction_record, \"card2\"),\n",
        "    get_feature_value(transaction_record, \"card3\"),\n",
        "    get_feature_value(transaction_record, \"card5\"),\n",
        "    get_feature_value(transaction_record, \"card_type_credit\"),\n",
        "    get_feature_value(transaction_record, \"card_type_debit\"),\n",
        "    get_feature_value(transaction_record, \"card_bank_american_express\"),\n",
        "    get_feature_value(transaction_record, \"card_bank_discover\"),\n",
        "    get_feature_value(transaction_record, \"card_bank_mastercard\"),\n",
        "    get_feature_value(transaction_record, \"card_bank_visa\"),\n",
        "]\n",
        "\n",
        "identity_response = featurestore_runtime.get_record(\n",
        "    FeatureGroupName=identity_feature_group_name, RecordIdentifierValueAsString=transaction_id\n",
        ")\n",
        "identity_record = identity_response[\"Record\"]\n",
        "id_test_data = [\n",
        "    get_feature_value(identity_record, \"id_01\"),\n",
        "    get_feature_value(identity_record, \"id_02\"),\n",
        "    get_feature_value(identity_record, \"id_03\"),\n",
        "    get_feature_value(identity_record, \"id_04\"),\n",
        "    get_feature_value(identity_record, \"id_05\"),\n",
        "]\n",
        "\n",
        "# Join all pieces for inference request.\n",
        "inference_request = []\n",
        "inference_request.extend(transaction_test_data[:])\n",
        "inference_request.extend(id_test_data[:])\n",
        "\n",
        "inference_request"
      ],
      "metadata": {
        "id": "e5B1KT3PRrgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "results = predictor.predict(\",\".join(inference_request), initial_args={\"ContentType\": \"text/csv\"})\n",
        "prediction = json.loads(results)\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "kqN1TrP4RxCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.delete_endpoint()"
      ],
      "metadata": {
        "id": "N3leQGG1RztW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_feature_group.delete()\n",
        "transaction_feature_group.delete()"
      ],
      "metadata": {
        "id": "LfexvgXcR4F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restore original boto3 version\n",
        "%pip install 'boto3=={}'.format(original_boto3_version)"
      ],
      "metadata": {
        "id": "VpJ-hVYIR-dh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}