{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0NXXVAVYsWpKp6LJm9Ux7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/ML540/blob/main/JFRIEDEL_USD_540_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#J Friedel  Assignment 2.1                      USD 540 Machine Learning Operations       "
      ],
      "metadata": {
        "id": "pugmGdaXgAws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Ingest homework data into data lake"
      ],
      "metadata": {
        "id": "MNXRubxFgL5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup All Workshop Dependencies FROM 01_setup_env_01_Setup_Dependencies"
      ],
      "metadata": {
        "id": "l0JGr0jQgS1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7YG5NK2f_Jy"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "97ilhh5Xgbmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pip\n",
        "!pip install --disable-pip-version-check -q pip --upgrade > /dev/null\n",
        "!pip install --disable-pip-version-check -q wrapt --upgrade > /dev/null"
      ],
      "metadata": {
        "id": "lmiAcQnignsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS CLI and AWS Python SDK (boto3)\n",
        "!pip install --disable-pip-version-check -q awscli boto3"
      ],
      "metadata": {
        "id": "NHcsTXrLgqKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SageMaker\n",
        "!pip install --disable-pip-version-check -q sagemaker\n",
        "!pip install --disable-pip-version-check -q smdebug\n",
        "!pip install --disable-pip-version-check -q sagemaker-experiments"
      ],
      "metadata": {
        "id": "VcrN69E6gx4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyAthena\n",
        "!pip install --disable-pip-version-check -q PyAthena"
      ],
      "metadata": {
        "id": "y-0-XkMtg0Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip\n",
        "!conda install -y zip"
      ],
      "metadata": {
        "id": "9VTRaaRog4fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matplotlib\n",
        "!pip install --disable-pip-version-check -q matplotlib"
      ],
      "metadata": {
        "id": "8Zi-vEHDg8YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seaborn\n",
        "!pip install --disable-pip-version-check -q seaborn"
      ],
      "metadata": {
        "id": "GMk1ApeLhAeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize\n",
        "!python --version"
      ],
      "metadata": {
        "id": "4qSARWRrhMdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "OIf2dvy6hOwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_dependencies_passed = True\n",
        "%store setup_dependencies_passed"
      ],
      "metadata": {
        "id": "PgiuduF3ha5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store"
      ],
      "metadata": {
        "id": "4lll666shebT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END of 01_setup_env_01_Setup_Dependencies"
      ],
      "metadata": {
        "id": "NADS7_achipC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# START 01_setup_env_02_Create_S3_Bucket"
      ],
      "metadata": {
        "id": "zJ5yD5_FhmlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create S3 Bucket\n",
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "session = boto3.session.Session()\n",
        "region = session.region_name\n",
        "sagemaker_session = sagemaker.Session()\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "\n",
        "s3 = boto3.Session().client(service_name=\"s3\", region_name=region)"
      ],
      "metadata": {
        "id": "fgFRgygEhfxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_s3_bucket_passed = False\n",
        "print(\"Default bucket: {}\".format(bucket))"
      ],
      "metadata": {
        "id": "hq9tOFaBhsck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify S3_BUCKET Bucket Creation\n",
        "from botocore.client import ClientError\n",
        "\n",
        "response = None\n",
        "\n",
        "try:\n",
        "    response = s3.head_bucket(Bucket=bucket)\n",
        "    print(response)\n",
        "    setup_s3_bucket_passed = True\n",
        "except ClientError as e:\n",
        "    print(\"[ERROR] Cannot find bucket {} in {} due to {}.\".format(bucket, response, e))"
      ],
      "metadata": {
        "id": "PFeKveQ0hv38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store setup_s3_bucket_passed"
      ],
      "metadata": {
        "id": "o3VbqYs-h1H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store"
      ],
      "metadata": {
        "id": "0Faw9uYqh31z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END  01_setup_env_02_Create_S3_Bucket"
      ],
      "metadata": {
        "id": "6z44vV0Ph8vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BEGIN 02_setup_datalake"
      ],
      "metadata": {
        "id": "mQ1kRCdKiBGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Pre-Requisites from an earlier notebook\n",
        "%store -r setup_dependencies_passed\n",
        "try:\n",
        "    setup_dependencies_passed\n",
        "except NameError:\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup Dependencies.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "print(setup_dependencies_passed)"
      ],
      "metadata": {
        "id": "3zikR-u7iFtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store -r setup_s3_bucket_passed\n",
        "try:\n",
        "    setup_s3_bucket_passed\n",
        "except NameError:\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup S3 Bucket.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "print(setup_s3_bucket_passed)"
      ],
      "metadata": {
        "id": "pyxgvOw-iJdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not setup_dependencies_passed:\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup Dependencies.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "if not setup_s3_bucket_passed:\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup S3 Bucket.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "bucket = sess.default_bucket()\n",
        "role = sagemaker.get_execution_role()\n",
        "region = boto3.Session().region_name\n",
        "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
        "\n",
        "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
      ],
      "metadata": {
        "id": "fEtUz9TciN5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set S3 Destination Location (Our Private S3 Bucket)\n",
        "s3_private_path_csv = \"s3://{}/assmt2-ds/csv\".format(bucket)\n",
        "print(s3_private_path_csv)"
      ],
      "metadata": {
        "id": "T35zCCE5iS-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store s3_p# Copy Data From the Public S3 Bucket to Private S3 Bucket in this Account\n",
        "!aws s3 cp \"dataset.csv\" $s3_private_path_csv/rivate_path_csv"
      ],
      "metadata": {
        "id": "TzXCONLNiWjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29qpkKI1iYta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List Files in our Private S3 Bucket in this Account\n",
        "print(s3_private_path_csv)"
      ],
      "metadata": {
        "id": "PlCV0wWoiejc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 ls $s3_private_path_csv/"
      ],
      "metadata": {
        "id": "KmJwbZpciizC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "display(\n",
        "    HTML(\n",
        "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-{}-{}/assmt2-ds/?region={}&tab=overview\">S3 Bucket</a></b>'.format(\n",
        "            region, account_id, region\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "xmqwG3zximC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Found in s3://sagemaker-us-east-1-244989531891/assmt2-ds/tsv/dataset.csv"
      ],
      "metadata": {
        "id": "MLhIeKRuircU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Variables for the Next Notebooks\n",
        "%store"
      ],
      "metadata": {
        "id": "iQ9cEnbdiu8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END 02_setup_datalake"
      ],
      "metadata": {
        "id": "-FINmwWkiyJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Set up the Athena query engine"
      ],
      "metadata": {
        "id": "22RZ8R5ji02b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "bucket = sess.default_bucket()\n",
        "role = sagemaker.get_execution_role()\n",
        "region = boto3.Session().region_name\n",
        "\n",
        "ingest_create_athena_table_csv_passed = False\n",
        "\n",
        "%store -r ingest_create_athena_db_passed\n",
        "\n",
        "try:\n",
        "    ingest_create_athena_db_passed\n",
        "except NameError:\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not create the Athena Database.\")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")"
      ],
      "metadata": {
        "id": "QKjr00tAi5Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ingest_create_athena_db_passed)"
      ],
      "metadata": {
        "id": "tchgtvSei9OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not ingest_create_athena_db_passed:\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not create the Athena Database.\")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "else:\n",
        "    print(\"[OK]\")"
      ],
      "metadata": {
        "id": "bogc-7BtjAK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store -r s3_private_path_csv\n",
        "\n",
        "try:\n",
        "    s3_private_path_csv\n",
        "except NameError:\n",
        "    print(\"*****************************************************************************\")\n",
        "    print(\"[ERROR] PLEASE RE-RUN THE PREVIOUS COPY TSV TO S3 NOTEBOOK ******************\")\n",
        "    print(\"[ERROR] THIS NOTEBOOK WILL NOT RUN PROPERLY. ********************************\")\n",
        "    print(\"*****************************************************************************\")\n",
        "\n",
        "print(s3_private_path_csv)"
      ],
      "metadata": {
        "id": "LxfGRhpMjCjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Athena Table from Local CSV Files\n",
        "# Import PyAthena\n",
        "from pyathena import connect\n",
        "\n",
        "# Set S3 staging directory -- this is a temporary directory used for Athena queries\n",
        "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)\n",
        "\n",
        "# Set Athena parameters\n",
        "database_name = \"dsAssmt2\"\n",
        "table_name_csv = \"assmt2_csv\"\n",
        "\n",
        "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)"
      ],
      "metadata": {
        "id": "hnWlB_SNjIAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually create the Athena database if it doesn't exist\n",
        "create_db_statement = \"CREATE DATABASE IF NOT EXISTS dsAssmt2\"\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(create_db_statement)\n",
        "print(\"[OK] Created database dsAssmt2 (or already exists)\")"
      ],
      "metadata": {
        "id": "HgUbTZV1jMuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_sql(statement, conn)"
      ],
      "metadata": {
        "id": "fh0WFEcRjQmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify The Table Has Been Created Succesfully\n",
        "statement = \"SHOW TABLES in {}\".format(database_name)\n",
        "\n",
        "df_show = pd.read_sql(statement, conn)\n",
        "df_show.head(5)"
      ],
      "metadata": {
        "id": "p61iRC9vjVBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query to preview data from the table\n",
        "AttributeErrorquery = f\"SELECT * FROM {database_name}.{table_name_csv} LIMIT 10\"\n",
        "df_preview = pd.read_sql(query, conn)\n",
        "df_preview"
      ],
      "metadata": {
        "id": "Lrrf2FNjjbDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL statement to execute\n",
        "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
        "         item_number int,\n",
        "         track_id string,\n",
        "         artists string,\n",
        "         album_name string,\n",
        "         track_name string,\n",
        "         popularity int,\n",
        "         duration_ms int,\n",
        "         explicit int,\n",
        "         danceability decimal(4,3),\n",
        "         energy decimal(5,4),\n",
        "         key int,\n",
        "         loudness decimal(5,3),\n",
        "         mode int,\n",
        "         speechiness decimal(5,4),\n",
        "         acousticness decimal(5,4),\n",
        "         instrumentalness float,\n",
        "         liveness decimal(5,4),\n",
        "         valence decimal(5,4),\n",
        "         tempo decimal(6,3),\n",
        "         time_signature int,\n",
        "         track_genre string\n",
        ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' LOCATION '{}'\n",
        "TBLPROPERTIES ('compressionType'='gzip', 'skip.header.line.count'='1')\"\"\".format(\n",
        "    database_name, table_name_csv, s3_private_path_csv\n",
        ")\n",
        "\n",
        "print(statement)"
      ],
      "metadata": {
        "id": "LvuUA-30jcu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is data looking like in df form\n",
        "import pandas as pd\n",
        "\n",
        "# Define your query\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM {database_name}.{table_name_csv}\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# Read the query result into a DataFrame\n",
        "df = pd.read_sql(query, conn)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "s0OZa9qIjnEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct columns with voided data for query exercises\n",
        "import numpy as np\n",
        "from scipy.stats import skewnorm\n",
        "\n",
        "# Replace \"key\" column with random integers from 1 to 11 (inclusive)\n",
        "df['key'] = np.random.randint(1, 12, size=len(df))\n",
        "\n",
        "# Fill 'explicit' column with 0s and 1s â€” 98% 0s, 2% 1s\n",
        "df['explicit'] = np.random.choice([0, 1], size=len(df), p=[0.98, 0.02])\n",
        "\n",
        "# Generate normally distributed values centered at 0.5 with std dev of 0.1\n",
        "danceability = np.random.normal(loc=0.5, scale=0.1, size=len(df))\n",
        "\n",
        "# Clip values to be within [0, 1] and round to 3 decimal places\n",
        "df['danceability'] = np.clip(danceability, 0, 1).round(3)\n",
        "\n",
        "df[\"popularity\"] = df[\"duration_ms\"]\n",
        "df[\"liveness\"] = df[\"tempo\"]\n",
        "df[\"time_signature\"] = df[\"track_genre\"]\n",
        "df[\"loudness\"] = df[\"mode\"]\n",
        "df[\"artists\"] = df[\"album_name\"]\n",
        "\n",
        "df[\"mode\"] = df[\"speechiness\"].astype(float).astype(int)\n",
        "\n",
        "df['speechiness'] = np.random.uniform(0.01, 0.1, size=len(df))\n",
        "\n",
        "# Parameters for skewed normal\n",
        "size = len(df)                     # Number of rows in your DataFrame\n",
        "skewness = -5                      # Negative = left skew (more values above 250,000)\n",
        "mean = 250000\n",
        "scale = 30000                     # Spread of the distribution\n",
        "\n",
        "# Generate skewed data\n",
        "skewed_data = skewnorm.rvs(a=skewness, loc=mean, scale=scale, size=size)\n",
        "\n",
        "# Clip values to stay within [100000, 350000]\n",
        "skewed_data = np.clip(skewed_data, 100000, 350000).astype(int)\n",
        "\n",
        "# Assign to 'duration_ms' column\n",
        "df[\"duration_ms\"] = skewed_data\n",
        "\n",
        "genres = [\n",
        "    \"acoustic\", \"black-metal\", \"brazil\", \"chill\", \"comedy\", \"dance-hall\", \"detroit-techno\",\n",
        "    \"drum-and-bass\", \"edm\", \"french\", \"german\", \"guitar\", \"house\", \"idm\", \"kids\", \"latino,metal\",\n",
        "    \"progressive-house\", \"r-n-b\", \"reggaeton\", \"rockabilly\", \"samba\", \"show-tunes\", \"sleep\",\n",
        "    \"spanish\", \"synth-pop\", \"world-music\"\n",
        "]\n",
        "\n",
        "df[\"track_genre\"] = np.random.choice(genres, size=len(df))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "bUVKpwS8jrCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. HOMEWORK Questions"
      ],
      "metadata": {
        "id": "ytKMPoLpjzuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          1. List artist, track_name, and popularity for songs that have a popularity greater than or equal to 99"
      ],
      "metadata": {
        "id": "5VAOKULmj21y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas\n",
        "# Convert 'popularity' to numeric (integer), coercing errors to NaN if necessary\n",
        "df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Now filter rows where popularity is >= 75\n",
        "df_high_popularity = df[df['popularity'] >= 75][['artists', 'track_name', 'popularity']]\n",
        "print(df_high_popularity)"
      ],
      "metadata": {
        "id": "7umF5Kkvj7dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL\n",
        "SELECT artists, track_name, popularity\n",
        "FROM dsAssmt2.assmt2_csv\n",
        "WHERE CAST(popularity AS INTEGER) >= 99"
      ],
      "metadata": {
        "id": "pi7m7y9tj_M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          2. List artists with an average popularity of 92, can the artists"
      ],
      "metadata": {
        "id": "hTOPzygpkDzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas\n",
        "# Ensure popularity is integer for accurate averaging\n",
        "df['popularity'] = df['popularity'].astype(int)\n",
        "\n",
        "# Group by 'artists' and calculate mean popularity\n",
        "artist_popularity = df.groupby('artists')['popularity'].mean().reset_index()\n",
        "\n",
        "# Filter artists with average popularity > 65\n",
        "filtered_artists = artist_popularity[artist_popularity['popularity'] > 65]\n",
        "\n",
        "# Sort by artist name (optional)\n",
        "filtered_artists = filtered_artists.sort_values(by='artists').reset_index(drop=True)\n",
        "\n",
        "# Display the result\n",
        "print(filtered_artists)"
      ],
      "metadata": {
        "id": "dlZBF8TPkHjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL\n",
        "ELECT\n",
        "    artists,\n",
        "    AVG(CAST(popularity AS INT)) AS average_popularity\n",
        "FROM\n",
        "    dsAssmt2.assmt2_csv\n",
        "GROUP BY\n",
        "    artists\n",
        "HAVING\n",
        "    AVG(CAST(popularity AS INT)) > 65\n",
        "ORDER BY\n",
        "    artists ASC;"
      ],
      "metadata": {
        "id": "xco1sZ-6kNsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          3. List the Top 10 genres with the highest average energy"
      ],
      "metadata": {
        "id": "xhSXb7n-kTZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas\n",
        "# Ensure 'energy' is numeric (if needed)\n",
        "df['energy'] = pd.to_numeric(df['energy'], errors='coerce')\n",
        "\n",
        "# Group by 'track_genre', calculate average 'energy', and sort\n",
        "top_energy_genres = (\n",
        "    df.groupby('track_genre')['energy']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values(by='energy', ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "# Display result\n",
        "print(top_energy_genres)"
      ],
      "metadata": {
        "id": "bkjieeQRkWvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL\n",
        "SELECT\n",
        "    track_genre,\n",
        "    AVG(CAST(energy AS DOUBLE)) AS avg_energy\n",
        "FROM\n",
        "    dsAssmt2.assmt2_csv\n",
        "GROUP BY\n",
        "    track_genre\n",
        "ORDER BY\n",
        "    avg_energy DESC\n",
        "LIMIT 10;"
      ],
      "metadata": {
        "id": "77lHeHLkkbFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          4. How many tracks is Bad Bunny on?"
      ],
      "metadata": {
        "id": "LT_h-HVjkftO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whitespace from 'artists' column\n",
        "df['artists'] = df['artists'].str.strip()\n",
        "\n",
        "# Sort the DataFrame by \"artists\"\n",
        "df_sorted = df.sort_values(by=\"artists\")\n",
        "\n",
        "# Filter rows where artists is exactly \"Bad Bunny\" and count them\n",
        "bad_bunny_count = df_sorted[df_sorted[\"artists\"] == \"Bad Bunny\"].shape[0]\n",
        "\n",
        "# Display the result\n",
        "print(f'Number of songs by Bad Bunny: {bad_bunny_count}')"
      ],
      "metadata": {
        "id": "SbJcGkkhkix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT COUNT(*) AS bad_bunny_count\n",
        "FROM dsAssmt2.assmt2_csv\n",
        "WHERE TRIM(artists) = 'Bad Bunny';"
      ],
      "metadata": {
        "id": "hFQHMGQzkm0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          5. Show the top 10 genres in terms of popularity, sorted by their most popular track"
      ],
      "metadata": {
        "id": "8p_HXHz5ksl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas\n",
        "# Compute top 10 genres by average energy\n",
        "top_genres = (\n",
        "    df.groupby(\"track_genre\")[\"energy\"]\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        "    .index.tolist()\n",
        ")\n",
        "\n",
        "# Filter df to only top genres\n",
        "df_top_genres = df[df[\"track_genre\"].isin(top_genres)]\n",
        "\n",
        "# For each genre, get the track with the highest popularity\n",
        "top_tracks_per_genre = (\n",
        "    df_top_genres.sort_values([\"track_genre\", \"popularity\"], ascending=[True, False])\n",
        "    .groupby(\"track_genre\")\n",
        "    .first()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Select relevant columns\n",
        "result = top_tracks_per_genre[[\"track_genre\", \"track_name\", \"popularity\"]]\n",
        "\n",
        "# Display the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "jcAmmik_kud6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL\n",
        "WITH top_genres AS (\n",
        "    SELECT track_genre\n",
        "    FROM your_database.your_table\n",
        "    GROUP BY track_genre\n",
        "    ORDER BY AVG(energy) DESC\n",
        "    LIMIT 10\n",
        "),\n",
        "ranked_tracks AS (\n",
        "    SELECT\n",
        "        track_genre,\n",
        "        track_name,\n",
        "        popularity,\n",
        "        ROW_NUMBER() OVER (PARTITION BY track_genre ORDER BY popularity DESC) as rank\n",
        "    FROM your_database.your_table\n",
        "    WHERE track_genre IN (SELECT track_genre FROM top_genres)\n",
        ")\n",
        "SELECT\n",
        "    track_genre,\n",
        "    track_name,\n",
        "    popularity\n",
        "FROM ranked_tracks\n",
        "WHERE rank = 1;"
      ],
      "metadata": {
        "id": "TzyyPEr2k2YD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}