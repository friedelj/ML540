{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZGrA8a7E9WIQjstEBmv1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/ML540/blob/main/JFriedel_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z61M7ta0fiF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#J Friedel  Assignment 2.1                      USD 540 Machine Learning Operations                                 26 May 2025"
      ],
      "metadata": {
        "id": "4jsmGX-XalW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Ingest homework data into data lake."
      ],
      "metadata": {
        "id": "9nI6sfxXfi4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup All Workshop Dependencies FROM 01_setup_env_01_Setup_Dependencies"
      ],
      "metadata": {
        "id": "ES4VMYdPau3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "WY6ghVt6a0RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "qVLmGLMXa6AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pip\n",
        "!pip install --disable-pip-version-check -q pip --upgrade > /dev/null\n",
        "!pip install --disable-pip-version-check -q wrapt --upgrade > /dev/null"
      ],
      "metadata": {
        "id": "RbvVJvfGbFV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS CLI and AWS Python SDK (boto3)\n",
        "!pip install --disable-pip-version-check -q awscli boto3"
      ],
      "metadata": {
        "id": "4QyzPZ9QbHK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SageMaker\n",
        "!pip install --disable-pip-version-check -q sagemaker\n",
        "!pip install --disable-pip-version-check -q smdebug\n",
        "!pip install --disable-pip-version-check -q sagemaker-experiments"
      ],
      "metadata": {
        "id": "30zyHkWCbKXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyAthena\n",
        "!pip install --disable-pip-version-check -q PyAthena"
      ],
      "metadata": {
        "id": "mZmCy7tCbNeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip\n",
        "!conda install -y zip"
      ],
      "metadata": {
        "id": "n1-e-PzsbTrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matplotlib\n",
        "!pip install --disable-pip-version-check -q matplotlib"
      ],
      "metadata": {
        "id": "_cfvoZdAbVqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seaborn\n",
        "!pip install --disable-pip-version-check -q seaborn"
      ],
      "metadata": {
        "id": "BBubqBCEbZfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize\n",
        "!python --version"
      ],
      "metadata": {
        "id": "_YcazJ-ibcJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "ueTsakQIbhJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_dependencies_passed = True\n",
        "%store setup_dependencies_passed"
      ],
      "metadata": {
        "id": "K56uPikYbrk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store"
      ],
      "metadata": {
        "id": "78DLR1wxbuXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END of 01_setup_env_01_Setup_Dependencies"
      ],
      "metadata": {
        "id": "RrjHXzrSbxY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# START 01_setup_env_02_Create_S3_Bucket"
      ],
      "metadata": {
        "id": "q40PWwI4b1gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P61bqERPfbkn"
      },
      "outputs": [],
      "source": [
        "# Create S3 Bucket\n",
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "session = boto3.session.Session()\n",
        "region = session.region_name\n",
        "sagemaker_session = sagemaker.Session()\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "\n",
        "s3 = boto3.Session().client(service_name=\"s3\", region_name=region)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setup_s3_bucket_passed = False\n",
        "print(\"Default bucket: {}\".format(bucket))"
      ],
      "metadata": {
        "id": "eOYzwEO4fsU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify S3_BUCKET Bucket Creation\n",
        "from botocore.client import ClientError\n",
        "\n",
        "response = None\n",
        "\n",
        "try:\n",
        "    response = s3.head_bucket(Bucket=bucket)\n",
        "    print(response)\n",
        "    setup_s3_bucket_passed = True\n",
        "except ClientError as e:\n",
        "    print(\"[ERROR] Cannot find bucket {} in {} due to {}.\".format(bucket, response, e))"
      ],
      "metadata": {
        "id": "ocLbthDsfwuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store setup_s3_bucket_passed"
      ],
      "metadata": {
        "id": "ELW_tu-fcW8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store"
      ],
      "metadata": {
        "id": "J_lKTLUDcba4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END  01_setup_env_02_Create_S3_Bucket"
      ],
      "metadata": {
        "id": "PErXxdzcchht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BEGIN 02_setup_datalake"
      ],
      "metadata": {
        "id": "vnREORjjcl4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Pre-Requisites from an earlier notebook\n",
        "%store -r setup_dependencies_passed\n",
        "try:\n",
        "    setup_dependencies_passed\n",
        "except NameError:\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup Dependencies.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "print(setup_dependencies_passed)"
      ],
      "metadata": {
        "id": "vYlSjASXcpqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store -r setup_s3_bucket_passed\n",
        "try:\n",
        "    setup_s3_bucket_passed\n",
        "except NameError:\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup S3 Bucket.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++\")\n",
        "print(setup_s3_bucket_passed)"
      ],
      "metadata": {
        "id": "heppeXb7cuKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not setup_dependencies_passed:\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup Dependencies.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "if not setup_s3_bucket_passed:\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL NOTEBOOKS IN THE SETUP FOLDER FIRST. You are missing Setup S3 Bucket.\")\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "bucket = sess.default_bucket()\n",
        "role = sagemaker.get_execution_role()\n",
        "region = boto3.Session().region_name\n",
        "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
        "\n",
        "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
      ],
      "metadata": {
        "id": "Ywz4K6L5czWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set S3 Destination Location (Our Private S3 Bucket)\n",
        "s3_private_path_csv = \"s3://{}/assmt2-ds/csv\".format(bucket)\n",
        "print(s3_private_path_csv)"
      ],
      "metadata": {
        "id": "0C8ZQ97Jc8Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store s3_private_path_csv"
      ],
      "metadata": {
        "id": "NXnle_UmdBgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy Data From the Public S3 Bucket to Private S3 Bucket in this Account\n",
        "!aws s3 cp \"dataset.csv\" $s3_private_path_csv/"
      ],
      "metadata": {
        "id": "QcLWOLtTdDQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List Files in our Private S3 Bucket in this Account\n",
        "print(s3_private_path_csv)"
      ],
      "metadata": {
        "id": "qxDsrwzhdJi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 ls $s3_private_path_csv/"
      ],
      "metadata": {
        "id": "PMTgL7pNdNR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "display(\n",
        "    HTML(\n",
        "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-{}-{}/assmt2-ds/?region={}&tab=overview\">S3 Bucket</a></b>'.format(\n",
        "            region, account_id, region\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "-NJKR3v_dQ9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Found in s3://sagemaker-us-east-1-244989531891/assmt2-ds/tsv/dataset.csv"
      ],
      "metadata": {
        "id": "Ey5izKcsdWWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Variables for the Next Notebooks\n",
        "%store"
      ],
      "metadata": {
        "id": "4r4CNSakdZRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END 02_setup_datalake"
      ],
      "metadata": {
        "id": "jrOX88G6dndf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Set up the Athena query engine"
      ],
      "metadata": {
        "id": "p6hA52qmgFpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "bucket = sess.default_bucket()\n",
        "role = sagemaker.get_execution_role()\n",
        "region = boto3.Session().region_name\n",
        "\n",
        "ingest_create_athena_table_csv_passed = False\n",
        "\n",
        "%store -r ingest_create_athena_db_passed\n",
        "\n",
        "try:\n",
        "    ingest_create_athena_db_passed\n",
        "except NameError:\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not create the Athena Database.\")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")"
      ],
      "metadata": {
        "id": "71SI-gf1gAW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ingest_create_athena_db_passed)"
      ],
      "metadata": {
        "id": "NgxUqXa4gO95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not ingest_create_athena_db_passed:\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(\"[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not create the Athena Database.\")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "else:\n",
        "    print(\"[OK]\")"
      ],
      "metadata": {
        "id": "R47lPuPggSUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%store -r s3_private_path_csv\n",
        "\n",
        "try:\n",
        "    s3_private_path_csv\n",
        "except NameError:\n",
        "    print(\"*****************************************************************************\")\n",
        "    print(\"[ERROR] PLEASE RE-RUN THE PREVIOUS COPY TSV TO S3 NOTEBOOK ******************\")\n",
        "    print(\"[ERROR] THIS NOTEBOOK WILL NOT RUN PROPERLY. ********************************\")\n",
        "    print(\"*****************************************************************************\")\n",
        "\n",
        "print(s3_private_path_csv)"
      ],
      "metadata": {
        "id": "ntpVHYSVgWC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Athena Table from Local CSV Files\n",
        "# Import PyAthena\n",
        "from pyathena import connect\n",
        "\n",
        "# Set S3 staging directory -- this is a temporary directory used for Athena queries\n",
        "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)\n",
        "\n",
        "# Set Athena parameters\n",
        "database_name = \"dsAssmt2\"\n",
        "table_name_csv = \"assmt2_csv\"\n",
        "\n",
        "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)"
      ],
      "metadata": {
        "id": "PBeg6xdAd6PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually create the Athena database if it doesn't exist\n",
        "create_db_statement = \"CREATE DATABASE IF NOT EXISTS dsAssmt2\"\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(create_db_statement)\n",
        "print(\"[OK] Created database dsAssmt2 (or already exists)\")"
      ],
      "metadata": {
        "id": "HJpnNKENeALB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL statement to execute\n",
        "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
        "         track_id string,\n",
        "         artists string,\n",
        "         album_name string,\n",
        "         track_name string,\n",
        "         popularity int,\n",
        "         duration_ms string,\n",
        "         explicit boolean,\n",
        "         danceability decimal(4,3),\n",
        "         energy decimal(5,4),\n",
        "         key int,\n",
        "         loudness decimal(5,3),\n",
        "         mode int,\n",
        "         speechiness decimal(5,4),\n",
        "         acousticness decimal(5,4),\n",
        "         instrumentalness float,\n",
        "         liveness decimal(5,4),\n",
        "         valence decimal(5,4),\n",
        "         tempo decimal(6,3),\n",
        "         time_signature int,\n",
        "         track_genre string\n",
        ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\\\n' LOCATION '{}'\n",
        "TBLPROPERTIES ('compressionType'='gzip', 'skip.header.line.count'='1')\"\"\".format(\n",
        "    database_name, table_name_csv, s3_private_path_csv\n",
        ")\n",
        "\n",
        "print(statement)"
      ],
      "metadata": {
        "id": "libF9jd-eDbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_sql(statement, conn)"
      ],
      "metadata": {
        "id": "ymEqxfCYeNCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify The Table Has Been Created Succesfully\n",
        "statement = \"SHOW TABLES in {}\".format(database_name)\n",
        "\n",
        "df_show = pd.read_sql(statement, conn)\n",
        "df_show.head(5)"
      ],
      "metadata": {
        "id": "wrEcklmxeQd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query to preview data from the table\n",
        "query = f\"SELECT * FROM {database_name}.{table_name_csv} LIMIT 10\"\n",
        "df_preview = pd.read_sql(query, conn)\n",
        "df_preview"
      ],
      "metadata": {
        "id": "vwxP9CDJeSx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8UHKsfmeYz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL statement to execute\n",
        "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
        "         item_nunber int,\n",
        "         track_id string,\n",
        "         artists string,\n",
        "         album_name string,\n",
        "         track_name string,\n",
        "         popularity int,\n",
        "         duration_ms string,\n",
        "         explicit int,\n",
        "         danceability decimal(4,3),\n",
        "         energy decimal(5,4),\n",
        "         key int,\n",
        "         loudness decimal(5,3),\n",
        "         mode int,\n",
        "         speechiness decimal(5,4),\n",
        "         acousticness decimal(5,4),\n",
        "         instrumentalness float,\n",
        "         liveness decimal(5,4),\n",
        "         valence decimal(5,4),\n",
        "         tempo decimal(6,3),\n",
        "         time_signature int,\n",
        "         track_genre string\n",
        ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' LOCATION '{}'\n",
        "TBLPROPERTIES ('compressionType'='gzip', 'skip.header.line.count'='1')\"\"\".format(\n",
        "    database_name, table_name_csv, s3_private_path_csv\n",
        ")\n",
        "\n",
        "print(statement)"
      ],
      "metadata": {
        "id": "9iYmMBmAHdHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#yyyyyyyyyyyyyyyyyyyyyy\n",
        "import pandas as pd\n",
        "\n",
        "# Define your query\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM {database_name}.{table_name_csv}\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# Read the query result into a DataFrame\n",
        "df = pd.read_sql(query, conn)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "Fsmkf9KAHcoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXCPWZgCHcHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct columns with voided data for query exercises\n",
        "import numpy as np\n",
        "from scipy.stats import skewnorm\n",
        "\n",
        "# Replace \"key\" column with random integers from 1 to 11 (inclusive)\n",
        "df['key'] = np.random.randint(1, 12, size=len(df))\n",
        "\n",
        "# Fill 'explicit' column with 0s and 1s — 98% 0s, 2% 1s\n",
        "df['explicit'] = np.random.choice([0, 1], size=len(df), p=[0.98, 0.02])\n",
        "\n",
        "# Generate normally distributed values centered at 0.5 with std dev of 0.1\n",
        "danceability = np.random.normal(loc=0.5, scale=0.1, size=len(df))\n",
        "\n",
        "# Clip values to be within [0, 1] and round to 3 decimal places\n",
        "df['danceability'] = np.clip(danceability, 0, 1).round(3)\n",
        "\n",
        "df[\"popularity\"] = df[\"duration_ms\"]\n",
        "df[\"liveness\"] = df[\"tempo\"]\n",
        "df[\"time_signature\"] = df[\"track_genre\"]\n",
        "df[\"loudness\"] = df[\"mode\"]\n",
        "\n",
        "df[\"mode\"] = df[\"speechiness\"].astype(float).astype(int)\n",
        "\n",
        "df['speechiness'] = np.random.uniform(0.01, 0.1, size=len(df))\n",
        "\n",
        "# Parameters for skewed normal\n",
        "size = len(df)                     # Number of rows in your DataFrame\n",
        "skewness = -5                      # Negative = left skew (more values above 250,000)\n",
        "mean = 250000\n",
        "scale = 30000                     # Spread of the distribution\n",
        "\n",
        "# Generate skewed data\n",
        "skewed_data = skewnorm.rvs(a=skewness, loc=mean, scale=scale, size=size)\n",
        "\n",
        "# Clip values to stay within [100000, 350000]\n",
        "skewed_data = np.clip(skewed_data, 100000, 350000).astype(int)\n",
        "\n",
        "# Assign to 'duration_ms' column\n",
        "df[\"duration_ms\"] = skewed_data\n",
        "\n",
        "genres = [\n",
        "    \"acoustic\", \"black-metal\", \"brazil\", \"chill\", \"comedy\", \"dance-hall\", \"detroit-techno\",\n",
        "    \"drum-and-bass\", \"edm\", \"french\", \"german\", \"guitar\", \"house\", \"idm\", \"kids\", \"latino,metal\",\n",
        "    \"progressive-house\", \"r-n-b\", \"reggaeton\", \"rockabilly\", \"samba\", \"show-tunes\", \"sleep\",\n",
        "    \"spanish\", \"synth-pop\", \"world-music\"\n",
        "]\n",
        "\n",
        "df[\"track_genre\"] = np.random.choice(genres, size=len(df))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "gwGjfTFGHpmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkNum2YcIgua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pg8Bmv0xgBmk"
      }
    }
  ]
}